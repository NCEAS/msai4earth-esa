{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde69939-82cf-437f-9acf-5c7d5dea582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import planetary_computer as pc\n",
    "import rioxarray as rioxr\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "from shapely.geometry import box\n",
    "\n",
    "from skimpy import clean_columns\n",
    "\n",
    "import raster_to_features as rf\n",
    "import data_sampling_workflow.sample_rasters as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47119d5-8ca5-405b-9041-485e0407de2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426dfdfd-53a7-4d59-87b9-550337f2f9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert reference class columns in df from CollectEarth \n",
    "# into a single ref_class column with values from 0 to 3\n",
    "def ref_class_column(df):\n",
    "\n",
    "    map_class = df.pl_class\n",
    "    ref_class = []\n",
    "\n",
    "    for i in map_class.index:\n",
    "        if df.category_non_iceplant_vegetation.loc[i] == 100:\n",
    "            ref_class.append(0)\n",
    "        elif df.category_iceplant.loc[i] == 100:\n",
    "            ref_class.append(1)\n",
    "        elif df.category_low_ndvi_impervious_surface.loc[i] == 100:\n",
    "            ref_class.append(2)\n",
    "        elif df.category_water.loc[i] == 100:\n",
    "            ref_class.append(3)\n",
    "        else:\n",
    "            ref_class[j]= 100\n",
    "            \n",
    "    return ref_class\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# make dictionary with points that need to be sampled from each scene\n",
    "def split_by_scene(pts, footprints):\n",
    "    \n",
    "    unchecked = list(pts.index)\n",
    "    pts_to_sample = {key: [] for key in footprints.id}\n",
    "\n",
    "    for itemid in footprints.id:\n",
    "        box = list(footprints[footprints.id == itemid].geometry)[0]\n",
    "\n",
    "        to_remove = []\n",
    "        for i in unchecked:\n",
    "            point = pts.iloc[i].geometry\n",
    "            if box.contains(point) == True:\n",
    "                pts_to_sample[itemid].append(i)\n",
    "                to_remove.append(i)\n",
    "\n",
    "        for i in to_remove:\n",
    "            unchecked.remove(i)\n",
    "    return pts_to_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cab21ff-560c-4aa6-830e-a0603123fbc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> column names have been cleaned\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m6\u001b[0m column names have been cleaned\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# load validation points\n",
    "fp = os.path.join(os.getcwd(), 'map_validation', 'validation_results_'+str(year)+'.csv' )\n",
    "validation_pts = clean_columns(sr.geodataframe_from_csv(fp = fp, \n",
    "                                                        lon_label = 'center_lon', \n",
    "                                                        lat_label = 'center_lat', \n",
    "                                                        crs = CRS.from_epsg(4326)))\n",
    "\n",
    "# clean data frame \n",
    "validation_pts['ref_class'] = ref_class_column(validation_pts)\n",
    "validation_pts = validation_pts.drop(['center_lon', 'center_lat', 'shape', 'size_m', 'sample_points',\n",
    "        'flagged', 'flagged_reason',  \n",
    "         'total_securewatch_dates', 'common_securewatch_date', \n",
    "         'validation_finished_yes_high_confidence',\n",
    "         'collection_time',\n",
    "         'category_low_ndvi_impervious_surface','category_non_iceplant_vegetation',\n",
    "         'category_iceplant', 'category_water', 'plotid'], axis =1)\n",
    "validation_pts = validation_pts.rename( \n",
    "    columns = {'pl_class':'map_class',\n",
    "               'validation_finished_no_low_confidence':'low_confidence'})\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# load NAIP scenes' footprints\n",
    "footprints = gpd.read_file(os.path.join(os.getcwd(), \n",
    "                                        'separating_naip_flights',\n",
    "                                        'naip_scenes_footprints',\n",
    "                                        'naip_scenes_footprints.shp'))\n",
    "footprints = footprints[footprints.year == str(year)]\n",
    "footprints.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758797f5-b4aa-49d6-b435-f586490b3168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# temporary folder for aux rasters\n",
    "folp = os.path.join(os.getcwd(),'temp','aux_naip_rasters')\n",
    "if os.path.exists(folp) == False:\n",
    "    os.mkdir(folp)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# create auxiliary canopy height rasters: min, max, and avg \n",
    "lidar_fps = rf.create_aux_canopyheight_rasters(year)\n",
    "\n",
    "# make dictionary with points that need to be sampled from each scene\n",
    "pts_to_sample = split_by_scene(validation_pts, footprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be8928f-1cb9-4f14-a094-fc4ad012dcdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "sampled_points = []\n",
    "for itemid in samples:\n",
    "#for itemid in ['ca_m_3412037_nw_10_060_20200607']:\n",
    "    if len(pts_to_sample[itemid]) > 0:\n",
    "        \n",
    "        to_sample = validation_pts.iloc[pts_to_sample[itemid]].geometry\n",
    "        # ---------------------------------------------------\n",
    "        # sample canopy height at point, and max, min and avg canopy height around point        \n",
    "        canopy_h_samples = []\n",
    "        for i, tag in zip(range(4),['', '_max', '_min', '_avg']):\n",
    "            aux_rast_r = rio.open(lidar_fps[i])\n",
    "            canopy_h_samples.append(sr.sample_raster_from_pts(to_sample, aux_rast_r, ['lidar'+tag]))        \n",
    "        # ---------------------------------------------------        \n",
    "        # sample spectral bands from NAIP\n",
    "        item = sr.get_item_from_id(itemid)\n",
    "        scene_rast_r = sr.get_raster_from_item(item)     \n",
    "\n",
    "        band_names = ['r', 'g', 'b', 'nir']\n",
    "        spectral_bands = sr.sample_raster_from_pts(to_sample, scene_rast_r, band_names).set_index(to_sample.index)\n",
    "        # ---------------------------------------------------        \n",
    "        # ---------------------------------------------------        \n",
    "        # sample max, min, avg and entrs from NAIP\n",
    "        to_sample_match = to_sample.to_crs(scene_rast_r.crs)\n",
    "        scene_rast = rioxr.open_rasterio(pc.sign(item.assets[\"image\"].href)) \n",
    "    \n",
    "        band_names.append('ndvi')\n",
    "        tags = ['_max', '_min', '_avg', '_entr']\n",
    "        window_cols = [band+tag for band in band_names for tag in tags]        \n",
    "\n",
    "        window_features = []\n",
    "        for i in range(len(to_sample_match)):\n",
    "#        for i in [0]:\n",
    "            pt = to_sample_match.iloc[[i]]\n",
    "            print(pt)\n",
    "            # clip scene to box around point\n",
    "            reduce_box = box(*(pt.iloc[0].buffer(6).bounds)) \n",
    "            rast = scene_rast.rio.clip_box(*reduce_box.bounds)\n",
    "            \n",
    "            # save auxiliary rasters for R,G,B,NIR: max,min,avg,entr\n",
    "            for i in range(4):\n",
    "                sr.max_min_avg_rasters(raster=rast, band=i+1, rast_name=band_names[i], n=3, folder_path=folp)\n",
    "                sr.entropy_raster(raster=rast, band=i+1, rast_name=band_names[i], n=3, folder_path=folp)\n",
    "            # ------------------------------\n",
    "            # make auxiliary NDVI of clipped scene\n",
    "            ndvi = sr.ndvi_xarray(rast)\n",
    "\n",
    "            # save auxiliary NDVI rasters: max,min,avg\n",
    "            sr.max_min_avg_rasters(rast_data=ndvi, \n",
    "                                   crs=rast.rio.crs, \n",
    "                                   transf=rast.rio.transform(), \n",
    "                                   rast_name=band_names[4], \n",
    "                                   n=3, \n",
    "                                   folder_path=folp)\n",
    "            \n",
    "            # adjust ndvi to entropy input types\n",
    "            ndvi = ndvi*100 +100\n",
    "            sr.entropy_raster(rast_data=ndvi.astype('uint8'), \n",
    "                              crs=rast.rio.crs, transf=rast.rio.transform(), \n",
    "                              rast_name=band_names[4], \n",
    "                              n=3, \n",
    "                              folder_path=folp)\n",
    "            # ---------------------------------------\n",
    "            # sample raster values for current point\n",
    "            samples = []\n",
    "            for col_name in window_cols:\n",
    "                fp = os.path.join(folp, col_name+'s.tif')\n",
    "                aux_rast_r = rio.open(fp)\n",
    "                sample = sr.sample_raster_from_pts(pt, aux_rast_r, [col_name])    \n",
    "                os.remove(fp)\n",
    "                samples.append(sample)       \n",
    "                \n",
    "            # ---------------------------------------\n",
    "            # Add all derived spectral data to pts dataframe\n",
    "            window_features.append(pd.concat(samples, axis = 1)) \n",
    "        # ---------------------------------------------------                    \n",
    "        # ---------------------------------------------------\n",
    "        # concatenate sampled data\n",
    "        window_features = pd.concat(window_features).set_index(to_sample_match.index)\n",
    "        lidar_bands = pd.concat(canopy_h_samples, axis=1).set_index(to_sample.index)\n",
    "        df = pd.concat([to_sample, spectral_bands, lidar_bands, window_features], axis=1)\n",
    "        # ---------------------------------------------------\n",
    "        # add date and naipid information\n",
    "        kwargs = {'year' : item.datetime.year,\n",
    "                  'month' : item.datetime.month,\n",
    "                  'day_in_year' : sr.day_in_year(item.datetime.day, item.datetime.month, item.datetime.year),\n",
    "                  'naip_id' : itemid}\n",
    "        df = df.assign(**kwargs)\n",
    "        # ---------------------------------------------------\n",
    "        sampled_points.append(gpd.GeoDataFrame(df))\n",
    "\n",
    "samples = pd.concat(sampled_points).sort_index()\n",
    "\n",
    "samples = pd.concat([samples, validation_pts.drop(['geometry'], axis=1)], axis=1)\n",
    "\n",
    "samples = samples[['geometry', \n",
    " 'r', 'g', 'b', 'nir', \n",
    " 'lidar', 'lidar_max', 'lidar_min', 'lidar_avg', \n",
    " 'year', 'month', 'day_in_year', \n",
    " 'naip_id', 'pl_which_raster',\n",
    " 'email', 'analysis_duration','low_confidence',\n",
    "  'map_class', 'ref_class']]\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9716f6b-2fb7-4082-8bc8-0ba269e56832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder = os.path.join(os.getcwd(), \n",
    "                      'validation_augmented_spectral_'+str(year))\n",
    "if os.path.exists(folder) == False:\n",
    "    os.mkdir(folder)\n",
    "\n",
    "samples.to_file(os.path.join(folder, 'validation_augmented_spectral_'+str(year))+'.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869f910-d5b2-4b37-842a-aea7b1114b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = rio.dtypes.get_minimum_dtype(rast)  \n",
    "sr.save_raster(rast, \n",
    "            os.path.join(folp,'naip_box.tif'), \n",
    "            [21,21],\n",
    "            4,\n",
    "            rast.rio.crs, \n",
    "            rast.rio.transform(), \n",
    "            dtype)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d937bce-fc67-441c-81ee-0aaf6c0ff2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rast.rio.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc3455-5027-4405-a391-dedc618ede50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
