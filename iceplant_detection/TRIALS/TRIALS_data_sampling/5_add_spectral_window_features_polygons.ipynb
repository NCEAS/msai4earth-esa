{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47801f9-46e8-4bb2-8980-dbc487f35af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import geopandas as gpd\n",
    "import rioxarray as rioxr\n",
    "import rasterio\n",
    "\n",
    "import sample_rasters as sr\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "from shapely.geometry import box\n",
    "\n",
    "import planetary_computer as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d916444f-1f0b-42e6-8f06-fcd8903eaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# ************* NOTEBOOK VARIABLES ******************\n",
    "\n",
    "itemids = pd.read_csv(sr.path_to_aoi_itemids_csv())\n",
    "\n",
    "# csv with the points for which to add spectral window features\n",
    "csv_name = 'model3070_train_2020.csv'\n",
    "fp = '/home/jovyan/msai4earth-esa/iceplant_detection/processing_results/model_3070/model3070_train_2020.csv'\n",
    "#fp = os.path.join(os.getcwd(),'temp',csv_name)\n",
    "all_pts = pd.read_csv(fp)\n",
    "\n",
    "entropy_r = 5\n",
    "\n",
    "# ***************************************************\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca68c8c9-5678-45b0-9f85-656d8a7336ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary folder for aux rasters\n",
    "folp = os.path.join(os.getcwd(),'temp','aux_naip_rasters')\n",
    "if os.path.exists(folp) == False:\n",
    "    os.mkdir(folp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d5beba-149e-4421-9211-47a8ec80c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED PROCESSINGss\n",
      "15.030651807785034\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time() # initial time trcker\n",
    "\n",
    "sampled_pts = [] # sampled pts from each poly are collected here\n",
    "\n",
    "N = len(itemids)  # counter to finish\n",
    "\n",
    "for i in range(len(itemids)):\n",
    "    # ---------------------------------------\n",
    "    # open raster reader for NAIP scene\n",
    "    itemid = itemids.itemid[i]\n",
    "    item = sr.get_item_from_id(itemid)    \n",
    "    href = pc.sign(item.assets[\"image\"].href)\n",
    "    naip_rast_r = rioxr.open_rasterio(href) \n",
    "\n",
    "    # ---------------------------------------\n",
    "    # find polygons for that NAIP scene\n",
    "    poly_fp = sr.path_to_polygons(itemids.iloc[i].aoi_name, itemids.iloc[i].year)\n",
    "    polys = gpd.read_file(poly_fp)\n",
    "    \n",
    "    # iterate through polygons in scene\n",
    "    for j in list(polys.id):\n",
    "        # ---------------------------------------\n",
    "        # find points in current polygon\n",
    "        pts_poly = all_pts.loc[ (all_pts['naip_id'] == itemid) & (all_pts['polygon_id'] == j)]\n",
    "        if len(pts_poly) !=0:\n",
    "\n",
    "            crs = CRS.from_string(pts_poly.pts_crs.iloc[0])\n",
    "            pts_poly_df = sr.geodataframe_from_csv(df = pts_poly, lon_label='x', lat_label='y', crs=crs)\n",
    "            pts_col = pts_poly_df.to_crs(naip_rast_r.rio.crs).geometry\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # create enlarged bounding box\n",
    "\n",
    "            # (this is ugly, but unfortunately the index and the polygon.id do not match in some files)\n",
    "            poly_index = polys.index[polys['id']==j].tolist()[0]                \n",
    "\n",
    "            poly = polys.geometry[poly_index]\n",
    "            reduce = gpd.GeoDataFrame({'geometry':[box(*poly.bounds)]}, crs=polys.crs)\n",
    "            reduce = reduce.to_crs(naip_rast_r.rio.crs) \n",
    "            poly = reduce.geometry[0]  # poly in scene's crs\n",
    "            \n",
    "            # ****** HERE *******\n",
    "            reduce_box = box(*(poly.buffer(entropy_r*2).bounds)) \n",
    "\n",
    "            # ---------------------------------------\n",
    "            # clip NAIP scene\n",
    "            rast = naip_rast_r.rio.clip_box(*reduce_box.bounds)\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # save auxiliary entropy rasters for R,G,B,NIR bands of clipped scene\n",
    "            band_names = ['r_', 'g_', 'b_', 'nir_']\n",
    "#            tags = ['_maxs', '_mins', '_avgs', '_entrs']\n",
    "            tags = ['_entrs']\n",
    "            window_fps = []\n",
    "            window_cols = []\n",
    "\n",
    "            for band_name, band_n in zip(band_names,range(1,5)):\n",
    "                rast_name = band_name + itemid + '_poly_'+str(j)\n",
    "              #  sr.max_min_avg_rasters(raster = rast, band=band_n, rast_name=rast_name, n=3, folder_path=folp)\n",
    "                sr.entropy_raster(raster = rast, band=band_n, rast_name=rast_name, n=entropy_r, folder_path=folp)\n",
    "\n",
    "                for tag in tags:\n",
    "                    window_fps.append(os.path.join(folp, rast_name + tag + '.tif'))        \n",
    "                    window_cols.append( band_name.replace('_','')+tag.replace('s',''))\n",
    "\n",
    "            # ------------------------------\n",
    "            # make auxiliary NDVI of clipped scene\n",
    "            ndvi = sr.ndvi_xarray(rast)\n",
    "\n",
    "            # make auxiliary NDVI entropy\n",
    "            band_names.append('ndvi_')\n",
    "            rast_name = 'ndvi_' + itemid + '_poly_'+str(j)\n",
    "            \n",
    "            #sr.max_min_avg_rasters(rast_data=ndvi, crs=rast.rio.crs, transf=rast.rio.transform(), rast_name=rast_name, n=3, folder_path=folp)\n",
    "            \n",
    "            # adjusting to entropy input types\n",
    "            ndvi = ndvi*100 +100\n",
    "            sr.entropy_raster(rast_data=ndvi.astype('uint8'), \n",
    "                              crs=rast.rio.crs, \n",
    "                              transf=rast.rio.transform(), \n",
    "                              rast_name=rast_name, \n",
    "                              n=entropy_r, \n",
    "                              folder_path=folp)\n",
    "\n",
    "            for tag in tags:\n",
    "                window_fps.append(os.path.join(folp, rast_name + tag + '.tif'))        \n",
    "                window_cols.append( 'ndvi'+tag.replace('s',''))\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # sample raster values for points in current polygon\n",
    "            samples = []\n",
    "            for fp, col_name in zip(window_fps, window_cols):\n",
    "                rast_r = rasterio.open(fp)\n",
    "                sample = sr.sample_raster_from_pts(pts_col, rast_r, [col_name])    \n",
    "                samples.append(sample)\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Add all derived spectral data to pts dataframe\n",
    "            new_features = pd.concat(samples, axis = 1)\n",
    "            pts = pd.concat([pts_poly, new_features.set_index(pts_poly_df.index)], axis=1)                \n",
    "\n",
    "            # -----------------------------\n",
    "            # collect all points from each polygon in the scene\n",
    "            sampled_pts.append(pts)\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # delete aux entropy rasters\n",
    "            for fp in window_fps:\n",
    "                os.remove(fp)\n",
    "            \n",
    "    # ---------------------------------------\n",
    "    # processing message\n",
    "    N = N-1                \n",
    "    print('REMAINING: ', N, 'scenes', end=\"\\r\")\n",
    "\n",
    "print('FINISHED PROCESSING')       \n",
    "\n",
    "     \n",
    "# ---------------------------------------\n",
    "# create data frame with all points\n",
    "sampled_pts= pd.concat(sampled_pts).sort_index()\n",
    "\n",
    "# ---------------------------------------\n",
    "# create max-min difference columns\n",
    "for band in band_names:\n",
    "    col_name = band + 'diff'\n",
    "    sampled_pts[col_name] = sampled_pts[band +'max'] - sampled_pts[band +'min']\n",
    "        \n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ab8d0-4ae3-47d9-96cb-66c4db05e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95aca0-10e1-4ad7-bc4c-c1b583940629",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pts = all_pts.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ea0fa-e002-4057-ba44-ebbf2e3e8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pts['r_entr5'] = sampled_pts.r_entr.iloc[:,1]\n",
    "all_pts['g_entr5'] = sampled_pts.g_entr.iloc[:,1]\n",
    "all_pts['b_entr5'] = sampled_pts.b_entr.iloc[:,1]\n",
    "all_pts['nir_entr5'] = sampled_pts.nir_entr.iloc[:,1]\n",
    "all_pts['ndvi_entr5'] = sampled_pts.ndvi_entr.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683517d-0eaa-4c90-ade2-902b6b7f390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a027fe4-d4d1-4553-b74f-1fb1c63195a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# clean dataframe columns\n",
    "all_pts = all_pts[['x', 'y', 'pts_crs', #  point location\n",
    "             'aoi', 'naip_id', 'polygon_id',  # sampling info\n",
    "             'r', 'r_max', 'r_min', 'r_diff', 'r_avg', 'r_entr', # spectral\n",
    "             'g', 'g_max', 'g_min', 'g_diff', 'g_avg', 'g_entr',\n",
    "             'b', 'b_max', 'b_min', 'b_diff', 'b_avg', 'b_entr',\n",
    "             'nir', 'nir_max', 'nir_min', 'nir_diff', 'nir_avg', 'nir_entr',\n",
    "             'ndvi', 'ndvi_max', 'ndvi_min', 'ndvi_diff', 'ndvi_avg', 'ndvi_entr',   \n",
    "             'year', 'month', 'day_in_year', # date\n",
    "#             'lidar', 'max_lidar', 'min_lidar', 'min_max_diff', 'avg_lidar', # lidar\n",
    "            'r_entr5','g_entr5', 'b_entr5', 'nir_entr5', 'ndvi_entr5',                          \n",
    "             'iceplant'\n",
    "             ]] \n",
    "\n",
    "# sampled_pts = sampled_pts.rename(columns={'max_lidar':'lidar_max',\n",
    "#                            'min_lidar':'lidar_min',\n",
    "#                            'min_max_diff':'lidar_diff',\n",
    "#                            'avg_lidar':'lidar_avg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01dd83f3-c69c-4943-b0f9-70b451079385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "## Save points with added spectral data\n",
    "fp = os.path.join(os.getcwd(),'temp', 'spectral_window_'+csv_name)\n",
    "all_pts.to_csv(fp, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459dab0-8ebd-44ad-b55a-534a074f2bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
