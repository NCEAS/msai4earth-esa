{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22271604-a3d1-4887-bb60-b519c6aad648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc   # garbage collector\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "import rioxarray as rioxr\n",
    "\n",
    "import dask_gateway\n",
    "import dask.array as da\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "# custom modules\n",
    "import raster_to_features as rf\n",
    "import data_sampling_workflow.sample_rasters as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9190925a-4975-4545-bad8-78498a2e8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year for which to predict iceplant locations\n",
    "year = 2020\n",
    "\n",
    "# whether to apply median filter to raster\n",
    "filter_rasters = True\n",
    "\n",
    "# whether to save rasters\n",
    "save_rasters = True\n",
    "prefix = 'LSWE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b6fc66-a393-4f8d-96e4-f75a9b77104e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# **************************************************************\n",
    "# CREATE AUXILIARY CANOPY HEIGHT FILES FOR GIVEN YEAR\n",
    "lidar_fps = rf.create_aux_canopyheight_rasters(year)\n",
    "#print('time to make auxiliary rasters: ', (time.time() - t0), 'seconds')\n",
    "\n",
    "# **************************************************************\n",
    "# open shapefile of SB coastal buffer and process it to use it for clipping\n",
    "fp = os.path.join(os.getcwd(), \n",
    "                  'separating_naip_flights', \n",
    "                  'SB_coastal_buffer', \n",
    "                  'SB_coastal_buffer.shp')\n",
    "coast = gpd.read_file(fp)\n",
    "coast_geo = coast.geometry.apply(mapping)\n",
    "\n",
    "# **************************************************************\n",
    "# load pre-trained random forest classifier\n",
    "# rfc = load('window_lidar_spectral_rfc.joblib') \n",
    "\n",
    "# **************************************************************\n",
    "# select the scene ids from given year that intersect the coastal buffer\n",
    "# the itemids of all scenes that intersect the coast were previously stored in a csv\n",
    "# scene_ids = pd.read_csv(os.path.join(os.getcwd(),\n",
    "#                                      'separating_naip_flights',\n",
    "#                                      'coastal_scenes_ids.csv'))\n",
    "# scene_ids = scene_ids.loc[scene_ids['year'] == year]\n",
    "# scene_ids = scene_ids.reset_index().itemid\n",
    "\n",
    "# **************************************************************\n",
    "# prepare folder to save rasters\n",
    "if save_rasters:\n",
    "    fp = os.path.join(os.getcwd(), 'processing_results')\n",
    "    if os.path.exists(fp) == False:\n",
    "        os.mkdir(fp)\n",
    "    if filter_rasters:\n",
    "        fp = os.path.join(fp, prefix+'_filter_clip_preds_' + str(year))\n",
    "    else:\n",
    "        fp = os.path.join(fp, prefix+'_clip_preds_' + str(year))\n",
    "    if os.path.exists(fp) == False:\n",
    "        os.mkdir(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d59019-71aa-45ce-a6ff-d17676ded967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca_m_3412039_nw_10_.6_20160616_20161004']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_ids = ['ca_m_3412039_nw_10_.6_20160616_20161004']\n",
    "scene_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae5874-c1cc-448f-a600-a221b2faf362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize DASK cluster\n",
    "cluster = dask_gateway.GatewayCluster()\n",
    "cluster.scale(15)\n",
    "\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9691e6-8ec5-416f-ba79-cab703242f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished adding canopy height features\n",
      "created R,G,B,NIR auxiliary rasters 11.102694988250732\n",
      "created NDVI auxiliary rasters 25.300808906555176\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "/home/jovyan/msai4earth-esa/iceplant_detection/temp/r_ca_m_3412039_nw_10_.6_20160616_20161004_entrs.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/file_manager.py:201\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 55\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<function open at 0x7f8262e891b0>, ('/home/jovyan/msai4earth-esa/iceplant_detection/temp/r_ca_m_3412039_nw_10_.6_20160616_20161004_entrs.tif',), 'r', (('sharing', False),)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mrasterio/_base.pyx:302\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_base.pyx:213\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_err.pyx:217\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: /home/jovyan/msai4earth-esa/iceplant_detection/temp/r_ca_m_3412039_nw_10_.6_20160616_20161004_entrs.tif: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m window_values \u001b[38;5;241m=\u001b[39m []    \n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp_aux \u001b[38;5;129;01min\u001b[39;00m window_fps:\n\u001b[0;32m--> 134\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[43mrioxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_rasterio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp_aux\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    135\u001b[0m     match_vector \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(match\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mmatch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    136\u001b[0m     window_values\u001b[38;5;241m.\u001b[39mappend(match_vector)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/rioxarray/_io.py:906\u001b[0m, in \u001b[0;36mopen_rasterio\u001b[0;34m(filename, parse_coordinates, chunks, cache, lock, masked, mask_and_scale, variable, group, default_name, decode_times, decode_timedelta, **open_kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m         manager \u001b[38;5;241m=\u001b[39m URIManager(rasterio\u001b[38;5;241m.\u001b[39mopen, filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39mopen_kwargs)\n\u001b[0;32m--> 906\u001b[0m     riods \u001b[38;5;241m=\u001b[39m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m     captured_warnings \u001b[38;5;241m=\u001b[39m rio_warnings\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# raise the NotGeoreferencedWarning if applicable\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/file_manager.py:183\u001b[0m, in \u001b[0;36mCachingFileManager.acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124;03m\"\"\"Acquire a file object from the manager.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    A new file is only opened if it has expired from the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m        An open file object, as returned by ``opener(*args, **kwargs)``.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     file, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/file_manager.py:207\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    205\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    206\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 207\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/rasterio/env.py:442\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/rasterio/__init__.py:277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m path \u001b[38;5;241m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 277\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    279\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[1;32m    280\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    281\u001b[0m     )\n",
      "File \u001b[0;32mrasterio/_base.pyx:304\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: /home/jovyan/msai4earth-esa/iceplant_detection/temp/r_ca_m_3412039_nw_10_.6_20160616_20161004_entrs.tif: No such file or directory"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# collect processing information for each scene\n",
    "times_pre = []\n",
    "times_class = []\n",
    "times_post = []\n",
    "processed = []\n",
    "reason = []\n",
    "veg_pixels = [] # number of pixels with ndwi<0.3 and ndwi>0.05\n",
    "n_pixels = []   # number of non-zero pixels in masked scene\n",
    "\n",
    "# counter for scenes queued for processing\n",
    "N = len(scene_ids)\n",
    "\n",
    "# ---------------------------------------\n",
    "# ---------------------------------------\n",
    "\n",
    "for itemid in scene_ids:\n",
    "    t_alpha = time.time()\n",
    "    \n",
    "    # ***********************************************************************************************\n",
    "    # *************************************** PRE-PROCESSING ****************************************\n",
    "    # open NAIP scene and clip to coast\n",
    "    t0 = time.time()\n",
    "    raster = rf.rioxr_from_itemid(itemid).rio.clip(coast_geo, coast.crs)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # select pixels with data (blacked out portions have 0 on all bands)\n",
    "    df = rf.raster_as_df(raster.to_numpy(), ['r','g','b','nir'])\n",
    "    df = df.loc[ (df['nir'] != 0) | (df['r'] != 0) | (df['g'] != 0) | (df['b'] != 0)]\n",
    "    n_pixels.append(df.shape[0])\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # stop if there's no data at intersection\n",
    "    if df.shape[0] == 0:\n",
    "        rf.finish_processing('no_data', processed, reason, times_pre, times_class, times_post, veg_pixels)\n",
    "        N = N-1\n",
    "\n",
    "    else:\n",
    "        # find vegetation pixels to go into model\n",
    "        # keep ndices of water and low-ndvi pixels\n",
    "        # add ndvi and ndwi features for each pixel\n",
    "        t0 = time.time()\n",
    "        is_veg, water_index, not_veg_index = rf.add_spectral_features(df, \n",
    "                                                                      ndwi_thresh = 0.3, \n",
    "                                                                      ndvi_thresh = 0.05) \n",
    "      # ---------------------------------------\n",
    "        # stop if there are no vegetation pixels at intersection\n",
    "        if is_veg.shape[0] == 0:\n",
    "            rf.finish_processing('no_veg', processed, reason, times_pre, times_class, times_post, veg_pixels)            \n",
    "            N = N-1\n",
    "\n",
    "        else:\n",
    "            processed.append('Y')\n",
    "            reason.append('processed')  \n",
    "            \n",
    "            # ---------------------------------------\n",
    "            # discard ndwi and add date features\n",
    "            is_veg.drop('ndwi', axis=1, inplace=True)\n",
    "            is_veg = rf.add_date_features(is_veg, rf.rioxr_from_itemid(itemid).datetime)\n",
    "\n",
    "    # ********************************************************************************************\n",
    "    # ******************************** ADD LIDAR FEATURES ****************************************\n",
    "            # resample canopy height layers as vectors    \n",
    "            lidar_values = []    \n",
    "            for fp_aux in lidar_fps:\n",
    "                match = sr.open_and_match(fp_aux, raster)\n",
    "                match_vector = match.to_numpy().reshape(match.shape[0]*match.shape[1])\n",
    "                lidar_values.append(match_vector)\n",
    "\n",
    "            df_lidar = pd.DataFrame(dict(zip(['lidar', 'lidar_max', 'lidar_min', 'lidar_avg'], lidar_values)))\n",
    "            df_lidar = df_lidar.assign(lidar_diff =  df_lidar['lidar_max'] - df_lidar['lidar_min'])\n",
    "\n",
    "            # ---------------------------------------\n",
    "            #  add canopy height features to vegetation dataframe \n",
    "            scene_features = pd.concat([is_veg, df_lidar.iloc[is_veg.index]], axis=1)\n",
    "            \n",
    "            # ---------------------------------------\n",
    "            # collect n of processed vegetation pts\n",
    "            veg_pixels.append(len(is_veg))       \n",
    "            print('finished adding canopy height features')\n",
    "\n",
    "            # ---------------------------------------\n",
    "            #free memory\n",
    "            del is_veg, df_lidar, match_vector, lidar_values\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    # *************************************************************************************************\n",
    "    # ******************************** CREATE R,G,B,NIR AUXILIARY RASTERS *****************************\n",
    "            t0 = time.time()\n",
    "            # make auxiliary spectral rasters from clipped NAIP (raster)\n",
    "            band_names = ['r_', 'g_', 'b_', 'nir_']\n",
    "            tags = ['_maxs', '_mins', '_avgs', '_entrs']\n",
    "            window_fps = []\n",
    "            window_cols = []\n",
    "\n",
    "            for name, band in zip(band_names,range(1,5)):\n",
    "                rast_name = name+itemid\n",
    "                sr.max_min_avg_rasters(raster=raster, band=band, rast_name=rast_name, n=3)\n",
    "             #   sr.entropy_raster(raster=raster, band=band, rast_name=rast_name, n=3)\n",
    "\n",
    "                for tag in tags:\n",
    "                    window_fps.append(os.path.join(os.getcwd(), 'temp', rast_name + tag + '.tif'))        \n",
    "                    window_cols.append( name.replace('_','')+tag.replace('s',''))\n",
    "            print('created R,G,B,NIR auxiliary rasters', time.time()-t0)\n",
    "\n",
    "    # ********************************************************************************************\n",
    "    # ******************************** CREATE NDVI AUXILIARY RASTERS *****************************\n",
    "            t0 = time.time()\n",
    "            # make auxiliary NDVI from clipped NAIP \n",
    "            ndvi = sr.ndvi_xarray(raster)\n",
    "            band_names.append('ndvi_')\n",
    "            rast_name = 'ndvi_'+itemid\n",
    "            sr.max_min_avg_rasters(raster=ndvi, rast_name=rast_name, n=3)\n",
    "            \n",
    "            # adjusting to entropy input types\n",
    "            ndvi = ndvi*100+100  \n",
    "            sr.entropy_raster(raster=ndvi.astype('uint8'), rast_name=rast_name, n=3)\n",
    "            \n",
    "            for tag in tags:\n",
    "                window_fps.append(os.path.join(os.getcwd(), 'temp', rast_name + tag + '.tif'))        \n",
    "                window_cols.append( 'ndvi'+tag.replace('s',''))\n",
    "\n",
    "            print('created NDVI auxiliary rasters', time.time()-t0)\n",
    "            #free memory\n",
    "            del ndvi\n",
    "            gc.collect()\n",
    "\n",
    "    # *******************************************************************************************\n",
    "    # *********************** EXTRACT FEATURES FROM AUXILIARY RASTERS ***************************\n",
    "    \n",
    "            window_values = []    \n",
    "            for fp_aux in window_fps:\n",
    "                match = rioxr.open_rasterio(fp_aux).squeeze()\n",
    "                match_vector = match.to_numpy().reshape(match.shape[0]*match.shape[1])\n",
    "                window_values.append(match_vector)\n",
    "                os.remove(fp_aux)\n",
    "\n",
    "            df_window = pd.DataFrame(dict(zip( window_cols, window_values)))\n",
    "\n",
    "            scene_features = pd.concat([scene_features, df_window.iloc[scene_features.index]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dbdf884-029a-4868-8114-94c5af442eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  10397,   20790,   20791,   20792,   20793,   20794,   20795,\n",
       "              20796,   20797,   31182,\n",
       "            ...\n",
       "            8216001, 8216002, 8216003, 8216004, 8216005, 8216006, 8216007,\n",
       "            8216008, 8216009, 8216010],\n",
       "           dtype='int64', length=5691)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_nadf = maxs_subset.isna()\n",
    "is_nadf = is_nadf[is_nadf.maxs_sr == True].index\n",
    "is_nadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b33958-7d38-48c0-b40f-b4ba79e8ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "            kwargs = {}\n",
    "            for band in band_names + ['lidar_']:\n",
    "                kwargs[band+'diff'] = scene_features[band+'max'] - scene_features[band+'min']\n",
    "\n",
    "            scene_features = scene_features.assign(**kwargs)\n",
    "\n",
    "            #free memory            \n",
    "            del df_window, window_values, match_vector, match\n",
    "            gc.collect()\n",
    "            \n",
    "            times_pre.append(time.time()-t0)\n",
    "       \n",
    "    # ******************************************************************************\n",
    "    # ******************************** ORDER FEATURES ****************************** \n",
    "\n",
    "            feature_order = [\n",
    "                         'r', 'r_max', 'r_min', 'r_diff', 'r_avg', 'r_entr',\n",
    "                         'g', 'g_max', 'g_min', 'g_diff', 'g_avg', 'g_entr',\n",
    "                         'b', 'b_max', 'b_min', 'b_diff', 'b_avg', 'b_entr',\n",
    "                         'nir', 'nir_max', 'nir_min', 'nir_diff', 'nir_avg', 'nir_entr',\n",
    "                         'ndvi', 'ndvi_max', 'ndvi_min', 'ndvi_diff', 'ndvi_avg', 'ndvi_entr',    # spectral\n",
    "                         'year', 'month', 'day_in_year', # date\n",
    "                         'lidar', 'lidar_max', 'lidar_min', 'lidar_diff', 'lidar_avg']\n",
    "\n",
    "            scene_features = scene_features[feature_order]\n",
    "            print('finished assembling features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b950697-22fe-40a7-bb93-36a2c64ba236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for itemid in scene_ids:\n",
    "    # ***********************************************************************************************\n",
    "    # *************************************** CLASSIFICATION ****************************************\n",
    "            # convert into dask.array and predict using model\n",
    "            da_pixels = da.from_array(np.array(scene_features), chunks=728802)\n",
    "            scene_preds = rfc.predict(da_pixels)\n",
    "            \n",
    "            # ---------------------------------------\n",
    "            t0 = time.time()\n",
    "            preds = scene_preds.compute()\n",
    "            times_class.append(time.time() - t0)\n",
    "            print('finished classification')\n",
    "\n",
    "    # ************************************************************************************************\n",
    "    # *************************************** POST-PROCESSING ****************************************\n",
    "            # recover pixel indices for iceplant classifications\n",
    "            t0 = time.time()\n",
    "            preds_df = pd.DataFrame(preds, \n",
    "                                 columns=['is_iceplant'], \n",
    "                                 index = scene_features.index)\n",
    "            is_iceplant_index = preds_df[preds_df.is_iceplant == 1].index.to_numpy()\n",
    "            non_iceplant_index = preds_df[preds_df.is_iceplant == 0].index.to_numpy()\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # reconstruct indices into image\n",
    "            indices = [non_iceplant_index,\n",
    "                       is_iceplant_index, \n",
    "                       not_veg_index,\n",
    "                       water_index]\n",
    "            values = [0,    # values assigned to pixels from each index\n",
    "                      1,\n",
    "                      2,\n",
    "                      3]\n",
    "            reconstruct = rf.indices_to_image(raster.shape[1], raster.shape[2], indices, values, back_value=100)\n",
    "            \n",
    "            # ---------------------------------------\n",
    "            # apply median 3x3 filter if needed\n",
    "            if filter:\n",
    "                reconstruct = median_filter(reconstruct, size=3)\n",
    "            \n",
    "            times_post.append(time.time() - t0)\n",
    "            print('finished post-processing')\n",
    "\n",
    "    # ************************************************************************************************\n",
    "    # *************************************** SAVE RASTERS *******************************************  \n",
    "            if save_rasters:\n",
    "                if filter_rasters:\n",
    "                    filename = prefix+'_filter_clip_preds_' + itemid + '.tif'\n",
    "                else:\n",
    "                    filename = prefix+'_clip_preds_' + itemid + '.tif'\n",
    "                \n",
    "                with rasterio.open(\n",
    "                    os.path.join(fp, filename),  # file path\n",
    "                    'w',           # w = write\n",
    "                    driver = 'GTiff', # format\n",
    "                    height = reconstruct.shape[0], \n",
    "                    width = reconstruct.shape[1],\n",
    "                    count = 1,  # number of raster bands in the dataset\n",
    "                    dtype = rasterio.uint8,\n",
    "                    crs = raster.rio.crs,\n",
    "                    transform = raster.rio.transform(),\n",
    "                ) as dst:\n",
    "                    dst.write(reconstruct.astype(rasterio.uint8), 1)\n",
    "                \n",
    "    # ************************************************************************************************\n",
    "    # ************************************ FINAL INFO MESSAGE ***************************************            \n",
    "            N = N-1\n",
    "            print('total time:', time.time() - t_alpha)\n",
    "            print('FINISHED: ', itemid)\n",
    "            print('REMAINING: ', N, 'scenes', end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1817eea-ec65-4e0a-b509-536e1e2e1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save times processed and itemids as dataframe\n",
    "D = { 'itemid': scene_ids,\n",
    "     'processed': processed,\n",
    "     'reason':reason,\n",
    "     'pre_times': times_pre,\n",
    "     'class_times' : times_class,\n",
    "     'post_times' : times_post, \n",
    "     'veg_pixels' : veg_pixels,\n",
    "     'processed_pix' : n_pixels }\n",
    "processing_df = pd.DataFrame(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00459eff-86c8-4bef-91b8-a23cf09a2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(os.getcwd(),'processing_results')\n",
    "if os.path.exists(fp) == False:\n",
    "    os.mkdir(fp)\n",
    "\n",
    "    \n",
    "if filter_rasters:\n",
    "    filename = prefix+'_filter_clip_processing_results_' + str(year) + '.csv'\n",
    "else:\n",
    "    filename = prefix+'_clip_processing_results_' + str(year) + '.csv'\n",
    "    \n",
    "processing_df.to_csv(os.path.join(fp, filename ), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1ba25-7b05-49eb-81ac-74be25e22cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1,4):   # delete aux canopy height rasters\n",
    "for i in range(1,4):\n",
    "    os.remove(lidar_fps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2be19-cc86-424a-9547-fc5f1d0e34e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
