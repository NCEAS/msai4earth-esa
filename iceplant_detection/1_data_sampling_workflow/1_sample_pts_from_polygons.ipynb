{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7857bfb-d31c-4c6b-937f-0f14b338b79c",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook extracts spectral and date features from NAIP images at random points within polygons in the 'polygons_form_naip_images' folder. These polygons are known iceplant and non iceplant locations within a specific NAIP image. \n",
    "\n",
    "\n",
    "Once the area of interest and years are specified, the notebook samples first polygons labeled as iceplant locations and then polygons labeled as non-iceplant locations. Three methods for sampling polygons are implemented in the `sample_rasters.py` module, all can be used in this notebook. These are:\n",
    "\n",
    "- sample a fixed fraction of pixels in each polygon,\n",
    "- sample a fixed fraction of the pixels in each polygon up to a maximum number of points, and\n",
    "- sample a fixed number of points from each polygon. \n",
    "\n",
    "Polygons vary significantly in size, so it is important to take into account that  sampling a fraction of the pixels in each polygon will likely result in an over-sampling of bigger polygons (most often those corresponding to non-iceplant locations), which in turn would greatly unbalance the training set towards one label. \n",
    "\n",
    "**NOTEBOOK VARIABLES:**\n",
    "\n",
    "- `aois` (array): These are the areas of interest where we collected the polygons we want to sample. Must be a subset of: `['campus_lagoon','carpinteria','gaviota','capitan','point_conception']`. \n",
    "\n",
    "- `years` (array): can be any subset of `[2012, 2014, 2016, 2018, 2020]`. If aoi = 'point_conception', then 2016 will not be included in the outcome since there are no NAIP images to sample from that year. \n",
    "\n",
    "- `ice_param` and `nonice_param` (str): determines which sampling method will be used to create the samples from the iceplant polygons and non-iceplant polygons, respectively. Must be one of 'fraction', 'sliding' or 'constant'. \n",
    "\n",
    "- `sample_fraction` (float in (0,1]): fraction of points to sample from each polygon\n",
    "\n",
    "- `max_sample` (int): maximum number of points to sample from a polygon\n",
    "\n",
    "- `const_sample` (int): constant number of points to sample from each polygon\n",
    "\n",
    "- `verbose` (bool): whether to print the stats of how many points were sampled per year and area of interest as the notebook runs\n",
    "\n",
    "- `save_stats` (bool): whether to save as a csv file the stats of how many points were sampled from each year and area of interest\n",
    "\n",
    "- `stats_csv_name` (str): where to save the stats from sampled points in the form \"file_name.csv\"\n",
    "\n",
    "- `save_pts` (bool): whether to save points as a csv file in temp folder  \n",
    "\n",
    "\n",
    "**OUTPUT:**\n",
    "\n",
    "The output is a data frame of points with the following features:\n",
    "\n",
    "- x, y: coordinates of point *p* \n",
    "- pts_crs: CRS of coordinates x, y\n",
    "- naip_id: itemid of the NAIP from which *p* was sampled from\n",
    "- polygon_id: id of the polygon from which *p* was sampled from\n",
    "- iceplant: whether point *p* corresponds to a confirmed iceplant location or a confirmed non-iceplant location (0 = non-iceplant, 1 = iceplant)\n",
    "- r, g, b, nir: Red, Green, Blue, and NIR values of NAIP scene with naip_id at coordinates of point *p*\n",
    "- ndvi: computed for each point using the Red and NIR bands\n",
    "- year, month, day_in_year: year, month, and day of the year when the NAIP image was collected\n",
    "- aoi: name of the area of interest where the points were sampled from\n",
    "\n",
    "\n",
    "The data frames are saved in the 'temp' folder as a csv file. Filenames have the structure: `aoi_points_year.csv'`\n",
    "The stats are saved in the current working directory with a specified file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d21b9a0-c9a8-44fb-9f3f-b07fa5cf2a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "import sample_rasters as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537b008-3fab-4251-94ad-177503dab258",
   "metadata": {},
   "source": [
    "# Specify notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a1d84e-4b5d-4606-aea2-a14747078dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# ************* NOTEBOOK VARIABLES ******************\n",
    "\n",
    "#aois = ['campus_lagoon','carpinteria','gaviota','point_conception']\n",
    "aois = ['capitan']\n",
    "\n",
    "# years = array of years, can be any subset from [2012, 2014, 2016, 2018, 2020]\n",
    "#years = [2012, 2014, 2016, 2018, 2020]\n",
    "years = ['2020']\n",
    "\n",
    "ice_param = 'constant'\n",
    "nonice_param = 'sliding'\n",
    "\n",
    "sample_fraction = 0.01\n",
    "max_sample = 10\n",
    "const_sample = 5\n",
    "\n",
    "# print stats as notebook runs\n",
    "verbose = True\n",
    "\n",
    "# save stats\n",
    "save_stats = False\n",
    "stats_csv_name = 'stats_sampling_pts_from_polygons.csv'\n",
    "\n",
    "#save points\n",
    "save_pts = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982a96a-4bc1-44e4-a28b-b35b207fdc3b",
   "metadata": {},
   "source": [
    "# Sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9994663-51ab-48ba-aace-c55861864785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sampling statistcs df\n",
    "stats = []\n",
    "\n",
    "# sample points\n",
    "for aoi in aois:\n",
    "    for year in years:\n",
    "        \n",
    "        #there's no data for Point Conception on 2016\n",
    "        if ('point_conception' != aoi) or (year != 2016):  \n",
    "            # open polygons\n",
    "            fp = sr.path_to_polygons(aoi, year)\n",
    "            polys = gpd.read_file(fp)\n",
    "            # -------------------------\n",
    "            # select iceplant polygons and sample sample_fraction of pts in each polygon \n",
    "            polys_ice = polys.loc[polys.iceplant == 1].reset_index(drop = True)\n",
    "            \n",
    "            pts_ice = sr.sample_naip_from_polys_no_warnings(polys = polys_ice,\n",
    "                                                            class_name = 'iceplant',\n",
    "                                                            itemid = polys.naip_id[0], \n",
    "                                                            param = ice_param,\n",
    "                                                            sample_fraction = sample_fraction,\n",
    "                                                            max_sample = max_sample,\n",
    "                                                            const_sample = const_sample)  \n",
    "\n",
    "            # -------------------------\n",
    "            # select non-iceplant polygons and sample sample_fraction of pts in each polygon,  \n",
    "            # but at most max_sample points \n",
    "            polys_nonice = polys.loc[polys.iceplant==0]\n",
    "            polys_nonice = polys_nonice.reset_index(drop=True)\n",
    "            \n",
    "            pts_nonice = sr.sample_naip_from_polys_no_warnings(polys = polys_nonice,\n",
    "                                                            class_name = 'iceplant',\n",
    "                                                            itemid = polys.naip_id[0], \n",
    "                                                            param = nonice_param,\n",
    "                                                            sample_fraction = sample_fraction,\n",
    "                                                            max_sample = max_sample,\n",
    "                                                            const_sample = const_sample)  \n",
    "            # -------------------------            \n",
    "            # assemble into single dataframe\n",
    "            pts = pd.concat([pts_ice, pts_nonice])\n",
    "            pts['aoi'] = aoi\n",
    "            \n",
    "            # add ndvi as feature\n",
    "            pts['ndvi'] = (pts.nir.astype('int16') - pts.r.astype('int16'))/(pts.nir.astype('int16') + pts.r.astype('int16'))\n",
    "\n",
    "            # -------------------------           \n",
    "            # save points as csv in temp folder  \n",
    "            if save_pts:\n",
    "                # create temp directory if needed\n",
    "                tmp_path = os.path.join(os.getcwd(),'temp')  \n",
    "                if not os.path.exists(tmp_path):\n",
    "                    os.mkdir(tmp_path)\n",
    "\n",
    "                fp = sr.path_to_spectral_pts(aoi, year)\n",
    "                pts.to_csv(fp, index=False)\n",
    "            \n",
    "            # -------------------------\n",
    "            # print sample statistics\n",
    "            if verbose:\n",
    "                print('************ '+aoi+ ' ' +str(year)+' ************')\n",
    "                sr.iceplant_proportions(pts.iceplant)\n",
    "                print( '---------------------------------------')\n",
    "                \n",
    "            # -------------------------\n",
    "            # keep track of statistics for saving\n",
    "            if save_stats:\n",
    "                n_ice =  pts_ice.shape[0]\n",
    "                n_nonice =  pts_nonice.shape[0]\n",
    "                total = n_ice + n_nonice\n",
    "                \n",
    "                stat = [aoi, \n",
    "                     year, \n",
    "                     str(round(n_nonice/n_ice,1))+':1', \n",
    "                     round(n_ice/total*100,2),\n",
    "                     round(n_nonice/total*100,2),\n",
    "                     n_ice,\n",
    "                     n_nonice\n",
    "                    ]\n",
    "                stats.append(stat)\n",
    "                \n",
    "# -------------------------   \n",
    "# save stats\n",
    "if save_stats:      \n",
    "    stats_df = pd.DataFrame(stats, \n",
    "                            columns=['aoi', 'year', 'ratio','perc_ice','perc_nonice','n_ice','n_nonice'])    \n",
    "    fp = os.path.join(os.getcwd(),  \n",
    "                      stats_csv_name)\n",
    "    stats_df.to_csv(fp, index=False)\n",
    "\n",
    "# -------------------------                    \n",
    "# final processing message               \n",
    "if verbose:                \n",
    "    print( 'non iceplant:iceplant ratio   ', sum(stats_df.n_nonice)/sum(stats_df.n_ice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9c3d6-6a0b-4d8b-aff9-073bb60ec26a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
