{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa89fce3-0450-4190-a2ba-3af09e400f3e",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook assembles all the csvs produced by the `3_add_lidar.ipynb` notebook into a single dataframe and saves it as a single csv. It also includes some statistics of the combined dataset. The notebook assumes all the posisble aois and years have been sampled and have a csv of points associated with them.\n",
    "\n",
    "**VARIABLES**\n",
    "\n",
    "`delete_files` (bool): whether to delete the individual csvs from which the final dataset is assembled\n",
    "\n",
    "**OUTPUT:**\n",
    "A single csv named 'samples_for_model.csv'. This contains all the points with all the features sampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a7815d-4fe8-4c41-b3d6-9c10af6995e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from utility import iceplant_proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e5fbb-1a44-4b8c-b7d4-0df68eef2e10",
   "metadata": {},
   "source": [
    "# Specify notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83bca10-f788-490f-9cff-bd1fc5a5cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# ************* NOTEBOOK VARIABLES ******************\n",
    "\n",
    "delete_files = False\n",
    "\n",
    "# print stats as notebook runs\n",
    "verbose = True\n",
    "\n",
    "# save stats\n",
    "write_stats = True\n",
    "# ***************************************************\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0ce63-481a-4fd0-a54c-a73713a447f1",
   "metadata": {},
   "source": [
    "## Paths to sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419e2adc-a221-4f89-9cf9-8228db7a6b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_spectral_points_csv(aoi, year):\n",
    "    \"\"\" Assembles a file path to file with points and ONLY spectral information from given aoi and year. \"\"\"\n",
    "    fp = os.path.join(os.getcwd(), \n",
    "                      'temp',\n",
    "                      aoi +'_points_'+str(year)+'.csv')\n",
    "    return fp            \n",
    "\n",
    "#---------------------\n",
    "\n",
    "def path_lidar_spectral_points_csv(aoi, year):\n",
    "    \"\"\" Assembles a file path to file with points and canopy height AND spectral information from given aoi and year. \"\"\"\n",
    "    fp = os.path.join(os.getcwd(), \n",
    "                      'temp',\n",
    "                      aoi +'_pts_spectral_lidar_'+str(year)+'.csv')\n",
    "    return fp            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d4a1a-7f56-4aec-8566-ddac8a30bf28",
   "metadata": {},
   "source": [
    "# Assemble data frame with all sampled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76317c0-f7f8-4969-be67-3b0cfcdefe68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lidar_years = [2016,2018,2020]\n",
    "spec_years = [2012,2014]\n",
    "aois = ['campus_lagoon','carpinteria','gaviota','point_conception']\n",
    "\n",
    "# *******************************************************************************\n",
    "# Open and concatenate csv files of points with canopy height + spectral info\n",
    "li = []\n",
    "for aoi in aois:\n",
    "    for year in lidar_years:\n",
    "        if ('point_conception' != aoi) or (year != 2016):  #there's no data for Point Conception on 2016\n",
    "            sample = pd.read_csv(path_lidar_spectral_points_csv(aoi,year))\n",
    "            li.append(sample)\n",
    "\n",
    "df_lidar = pd.concat(li, axis=0)\n",
    "\n",
    "# only keep points with non-negative canopy height values\n",
    "df_lidar = df_lidar.loc[(df_lidar[\"lidar\"] >= 0) & \n",
    "                        (df_lidar[\"max_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"min_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"avg_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"min_max_diff\"] >= 0)\n",
    "                       ]\n",
    "\n",
    "# *******************************************************************************\n",
    "# Open and concatenate csv files of points with canopy height + spectral info\n",
    "li = []\n",
    "for aoi in aois:\n",
    "    for year in spec_years:\n",
    "            sample = pd.read_csv(path_spectral_points_csv(aoi,year))\n",
    "            li.append(sample)\n",
    "\n",
    "df_spec = pd.concat(li, axis=0)\n",
    "\n",
    "# fill in canopy height columns wit NaN\n",
    "df_spec['lidar'] = np.nan\n",
    "df_spec['max_lidar'] = np.nan\n",
    "df_spec['min_lidar'] = np.nan\n",
    "df_spec['avg_lidar'] = np.nan\n",
    "df_spec['min_max_diff'] = np.nan\n",
    "\n",
    "# *******************************************************************************\n",
    "# concatenate both data frames and clean index and columns\n",
    "df = pd.concat([df_lidar,df_spec], axis=0)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = df[['x', 'y', 'pts_crs', #  point location\n",
    "         'aoi','naip_id', 'polygon_id',  # sampling info\n",
    "         'r','g','b','nir','ndvi',     # spectral\n",
    "         'year','month','day_in_year', # date\n",
    "         'lidar', 'max_lidar', 'min_lidar', 'min_max_diff', 'avg_lidar', # lidar\n",
    "         'iceplant'\n",
    "         ]] \n",
    "\n",
    "# *******************************************************************************\n",
    "# save data frame as csv\n",
    "df.to_csv(os.path.join(os.getcwd(),'samples_for_model.csv'), index=False)\n",
    "\n",
    "# *******************************************************************************\n",
    "# Delete individual files\n",
    "if delete_files == True:\n",
    "    for aoi in aois:\n",
    "        for year in years:\n",
    "            if year in spec_years:\n",
    "                os.remove(path_spectral_points_csv(aoi,year))\n",
    "            if year in lidar_years:\n",
    "                if ('point_conception' != aoi) or (year != 2016):  #there's no data for Point Conception on 2016\n",
    "                    os.remove(path_lidar_spectral_points_csv(aoi,year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24726e-83a8-4c5e-8f22-b863d89cbafd",
   "metadata": {},
   "source": [
    "# Statistics about data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c2138e-3c47-4c5f-85ef-95a659d0c836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# points sampled: 489415\n",
      "\n",
      "\n",
      "no-iceplant:iceplant ratio   1.8:1\n",
      "\n",
      "\n",
      "          counts  percentage\n",
      "iceplant                    \n",
      "0         313132       63.98\n",
      "1         176283       36.02\n",
      "\n",
      "\n",
      "Points sampled per area of interest\n",
      "point_conception    191958\n",
      "campus_lagoon       132326\n",
      "carpinteria         102089\n",
      "gaviota              63042\n",
      "\n",
      "\n",
      "Points sampled per year\n",
      "2020    149382\n",
      "2018    121902\n",
      "2014     89163\n",
      "2012     77224\n",
      "2016     51744\n",
      "\n",
      "\n",
      "# NAIP scenes sampled: 19\n",
      "Points sampled per NAIP scene\n",
      "ca_m_3412037_nw_10_060_20200607             80913\n",
      "ca_m_3411934_sw_11_060_20180722_20190209    54945\n",
      "ca_m_3412037_nw_10_1_20140603_20141030      50837\n",
      "ca_m_3412037_nw_10_1_20120518_20120730      31957\n",
      "ca_m_3412037_nw_10_060_20180913_20190208    28251\n",
      "ca_m_3411936_se_11_060_20200521             27930\n",
      "ca_m_3411934_sw_11_060_20200521             25293\n",
      "ca_m_3411936_se_11_060_20180724_20190209    23820\n",
      "ca_m_3411934_sw_11_.6_20160713_20161004     20278\n",
      "ca_m_3411936_se_11_.6_20160713_20161004     18413\n",
      "ca_m_3411934_sw_11_1_20120505_20120730      18378\n",
      "ca_m_3411936_se_11_1_20120505_20120730      16568\n",
      "ca_m_3411936_se_11_1_20140901_20141030      15358\n",
      "ca_m_3412039_nw_10_060_20200522             15246\n",
      "ca_m_3412039_nw_10_060_20180724_20190209    14886\n",
      "ca_m_3411934_sw_11_1_20140601_20141030      13432\n",
      "ca_m_3412039_nw_10_.6_20160616_20161004     13053\n",
      "ca_m_3412039_nw_10_1_20120518_20120730      10321\n",
      "ca_m_3412039_nw_10_1_20140603_20141030       9536\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sep = '\\n\\n\\n'\n",
    "\n",
    "n_sample =  '# points sampled: '+ str(df.shape[0]) + sep\n",
    "\n",
    "# ----------------------------\n",
    "# ratios and percentages of iceplant vs non-iceplant\n",
    "unique, counts = np.unique(df.iceplant, return_counts=True)\n",
    "icep_ratio = 'no-iceplant:iceplant ratio   '+ str(round(counts[0]/counts[1],1))+':1' +sep\n",
    "\n",
    "n = df.iceplant.shape[0]\n",
    "perc = [round(counts[0]/n*100,2), round(counts[1]/n*100,2)]\n",
    "counts_percents = pd.DataFrame({'iceplant':unique,\n",
    "         'counts':counts,\n",
    "         'percentage':perc}).set_index('iceplant')\n",
    "counts_percents = counts_percents.to_string() + sep\n",
    "\n",
    "# ----------------------------\n",
    "# Number of points by area of interest\n",
    "counts_aoi = 'Points sampled per area of interest\\n' + df.aoi.value_counts().to_string() + sep\n",
    "\n",
    "# Number of points by year\n",
    "counts_year = 'Points sampled per year\\n'+ df.year.value_counts().to_string() + sep\n",
    "\n",
    "# Number of points by NAIP scene\n",
    "counts_naipid = '# NAIP scenes sampled: '+ str(len(df.naip_id.value_counts()))+'\\nPoints sampled per NAIP scene\\n' +  df.naip_id.value_counts().to_string() + sep\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# assemble all stats into string\n",
    "stats = n_sample + icep_ratio + counts_percents + counts_aoi + counts_year + counts_naipid\n",
    "\n",
    "# *******************************************************************************\n",
    "if write_stats:\n",
    "    with open(os.path.join(os.getcwd(),'stats.txt'), 'a') as f:\n",
    "        f.write(stats)\n",
    "        f.close()\n",
    "\n",
    "# *******************************************************************************\n",
    "if verbose:\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0a95a-36d7-4572-862a-0d4b2dd37cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
