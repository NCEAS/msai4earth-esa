{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa89fce3-0450-4190-a2ba-3af09e400f3e",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook assembles all the csv files produced by the `3_add_canopy_height_features.ipynb` notebook into a single dataframe of samples for the iceplant detection model. While assembling the files we also:\n",
    "- delete any sample points with a negative canopy height feature from the dataset. Canopy height features are: lidar, max_lidar, min_lidar, avg_lidar and min_max_diff. \n",
    "- add NaN as the value for all canopy height features for points sampled in 2012 and 2014. \n",
    "\n",
    "To run the notebook, it is necessary to have a csv file with points for each year and area of interest. The output is either a single csv file with all the combined data samples, or two csv files corresponding to splitting the samples into train and test sets. The training set is done by sampling the same percentage of points per scene. This is an effort to keep the training and test sets unbiased toward scenes with more points sampled. The notebook also prints and saves some statistics of the resulting dataset(s).\n",
    "\n",
    "**VARIABLES**\n",
    "\n",
    "`delete_files` (bool): whether to delete the individual csv files from which the final dataset is assembled\n",
    "\n",
    "`verbose` (bool): whether to print in the console the stats of how many points were sampled per year and area of interest\n",
    "\n",
    "`write_stats` (bool):  whether to save as a text file stats about the distribution of sample points in the dataset(s)\n",
    "\n",
    "`split` (bool): if split=True, then the dataset is split into train and test sets, keeping the same proportion of points per scene. \n",
    "\n",
    "`test_size` (float in (0,1)): percentage of data samples that should go into the test set. The notebook will sample this percentage of test points from each scene. \n",
    "\n",
    "Add: new variables\n",
    "\n",
    "**OUTPUT:**\n",
    "- If split=False: a single csv file named 'samples_for_model.csv'. This contains all the points with all the features sampled. \n",
    "- If split=True: two csv files, one for the train set and another for the test set.\n",
    "- If write_stats=True: A text file named samples_for_model_stats.txt. Nothing is generated if write_stats=False.\n",
    "\n",
    "All files are saved in the current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a7815d-4fe8-4c41-b3d6-9c10af6995e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e5fbb-1a44-4b8c-b7d4-0df68eef2e10",
   "metadata": {},
   "source": [
    "# Specify notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83bca10-f788-490f-9cff-bd1fc5a5cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# ************* NOTEBOOK VARIABLES ******************\n",
    "\n",
    "delete_files = True\n",
    "\n",
    "# print stats as notebook runs\n",
    "verbose = True\n",
    "\n",
    "# save stats\n",
    "write_stats = True\n",
    "\n",
    "split = True\n",
    "\n",
    "test_size = 0.3\n",
    "# ***************************************************\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0ce63-481a-4fd0-a54c-a73713a447f1",
   "metadata": {},
   "source": [
    "## Paths to sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419e2adc-a221-4f89-9cf9-8228db7a6b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_spectral_points_csv(aoi, year):\n",
    "    \"\"\" Assembles a file path to file with points and ONLY spectral information from given aoi and year. \"\"\"\n",
    "    fp = os.path.join(os.getcwd(), \n",
    "                      'temp',\n",
    "                      aoi +'_points_'+str(year)+'.csv')\n",
    "    return fp            \n",
    "\n",
    "#---------------------\n",
    "\n",
    "def path_lidar_spectral_points_csv(aoi, year):\n",
    "    \"\"\" Assembles a file path to file with points and canopy height AND spectral information from given aoi and year. \"\"\"\n",
    "    fp = os.path.join(os.getcwd(), \n",
    "                      'temp',\n",
    "                      aoi +'_pts_spectral_lidar_'+str(year)+'.csv')\n",
    "    return fp            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d4a1a-7f56-4aec-8566-ddac8a30bf28",
   "metadata": {},
   "source": [
    "## Assemble data frame with all sampled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3395de-90d4-4d59-a897-c5ca04a504d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lidar_years = [2016,2018,2020]\n",
    "spec_years = [2012,2014]\n",
    "aois = ['campus_lagoon','carpinteria','gaviota','point_conception']\n",
    "\n",
    "# *******************************************************************************\n",
    "# Open and concatenate csv files of points with canopy height + spectral info\n",
    "li = []\n",
    "for aoi in aois:\n",
    "    for year in lidar_years:\n",
    "        if ('point_conception' != aoi) or (year != 2016):  #there's no data for Point Conception on 2016\n",
    "            sample = pd.read_csv(path_lidar_spectral_points_csv(aoi,year))\n",
    "            li.append(sample)\n",
    "\n",
    "df_lidar = pd.concat(li, axis=0)\n",
    "\n",
    "# only keep points with non-negative canopy height values\n",
    "df_lidar = df_lidar.loc[(df_lidar[\"lidar\"] >= 0) & \n",
    "                        (df_lidar[\"max_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"min_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"avg_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"min_max_diff\"] >= 0)\n",
    "                       ]\n",
    "\n",
    "# *******************************************************************************\n",
    "# Open and concatenate csv files of points with canopy height + spectral info\n",
    "li = []\n",
    "for aoi in aois:\n",
    "    for year in spec_years:\n",
    "            sample = pd.read_csv(path_spectral_points_csv(aoi,year))\n",
    "            li.append(sample)\n",
    "\n",
    "df_spec = pd.concat(li, axis=0)\n",
    "\n",
    "# fill in canopy height columns wit NaN\n",
    "df_spec['lidar'] = np.nan\n",
    "df_spec['max_lidar'] = np.nan\n",
    "df_spec['min_lidar'] = np.nan\n",
    "df_spec['avg_lidar'] = np.nan\n",
    "df_spec['min_max_diff'] = np.nan\n",
    "\n",
    "# *******************************************************************************\n",
    "# concatenate both data frames and clean index and columns\n",
    "samples = pd.concat([df_lidar,df_spec], axis=0)\n",
    "\n",
    "samples.reset_index(drop=True, inplace=True)\n",
    "\n",
    "samples = samples[['x', 'y', 'pts_crs', #  point location\n",
    "         'aoi','naip_id', 'polygon_id',  # sampling info\n",
    "         'r','g','b','nir','ndvi',     # spectral\n",
    "         'year','month','day_in_year', # date\n",
    "         'lidar', 'max_lidar', 'min_lidar', 'min_max_diff', 'avg_lidar', # lidar\n",
    "         'iceplant'\n",
    "         ]] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73390f-fff9-4cbc-a935-92112e197b0c",
   "metadata": {},
   "source": [
    "## If split == True: split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53cfde5-6271-4034-8c3f-14b18a2e8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    # initialize empty train and test lists\n",
    "    all_train = []\n",
    "    all_test = []\n",
    "\n",
    "    X_labels = samples.columns.drop('iceplant')     # save label names\n",
    "\n",
    "    aois = samples.aoi.unique()     # list of aois\n",
    "\n",
    "    for aoi in aois:\n",
    "\n",
    "        # retrieve all scenes from  aoi\n",
    "        in_aoi = samples[samples.aoi == aoi]    \n",
    "        scenes = in_aoi.naip_id.unique()\n",
    "\n",
    "        for scene in scenes:\n",
    "            # get all pts in scene\n",
    "            in_scene = in_aoi[in_aoi.naip_id == scene]\n",
    "\n",
    "            # sample test_size fraction of pts in scene for testing\n",
    "            # keep same iceplant/non-ice plant proportion in test an train sets\n",
    "            X = np.array(in_scene.drop('iceplant', axis = 1))\n",
    "            y = np.array(in_scene['iceplant'])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                                test_size = test_size, \n",
    "                                                                random_state = 42)   # fix random seed\n",
    "            # reassemble train set into data frame\n",
    "            train = pd.DataFrame(X_train, columns = X_labels)\n",
    "            train['iceplant'] = y_train\n",
    "\n",
    "            # reassemble test set into data \n",
    "            test = pd.DataFrame(X_test, columns = X_labels)\n",
    "            test['iceplant'] = y_test\n",
    "\n",
    "            # add to rest of train/test pts\n",
    "            all_train.append(train)\n",
    "            all_test.append(test)\n",
    "    \n",
    "    train = pd.concat(all_train, ignore_index=True)\n",
    "    test = pd.concat(all_test, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9107185-373d-4ead-bdf1-d5a2f90179e3",
   "metadata": {},
   "source": [
    "## Save data samples file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31059e58-8108-4e37-a3d6-5fc0c07024d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    train.to_csv(os.path.join(os.getcwd(),'train_set.csv'), index=False)\n",
    "    test.to_csv(os.path.join(os.getcwd(),'test_set.csv'), index=False)\n",
    "    \n",
    "else:\n",
    "    samples.to_csv(os.path.join(os.getcwd(),'samples_for_model.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ef100-2d99-4a8f-85a8-98ca9ac49882",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Delete individual csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2082a9e8-3c7b-47d7-9617-bbba3f28cc76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_files == True:\n",
    "    for aoi in aois:\n",
    "        for year in [2012,2014,2016,2018,2020]:\n",
    "            if year in spec_years:\n",
    "                os.remove(path_spectral_points_csv(aoi,year))\n",
    "            if year in lidar_years:\n",
    "                if ('point_conception' != aoi) or (year != 2016):  #there's no data for Point Conception on 2016\n",
    "                    os.remove(path_lidar_spectral_points_csv(aoi,year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24726e-83a8-4c5e-8f22-b863d89cbafd",
   "metadata": {},
   "source": [
    "## Statistics about data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c2138e-3c47-4c5f-85ef-95a659d0c836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** INFORMATION ABOUT COMPLETE DATASET ******\n",
      "\n",
      "\n",
      "total # of points sampled: 489415\n",
      "\n",
      "\n",
      "no-iceplant:iceplant ratio   1.8:1\n",
      "\n",
      "\n",
      "          counts  percentage\n",
      "iceplant                    \n",
      "0         313132       63.98\n",
      "1         176283       36.02\n",
      "\n",
      "\n",
      "Points sampled per area of interest\n",
      "point_conception    191958\n",
      "campus_lagoon       132326\n",
      "carpinteria         102089\n",
      "gaviota              63042\n",
      "\n",
      "\n",
      "Points sampled per year\n",
      "2020    149382\n",
      "2018    121902\n",
      "2014     89163\n",
      "2012     77224\n",
      "2016     51744\n",
      "\n",
      "\n",
      "# NAIP scenes sampled: 19\n",
      "Points sampled per NAIP scene\n",
      "ca_m_3412037_nw_10_060_20200607             80913\n",
      "ca_m_3411934_sw_11_060_20180722_20190209    54945\n",
      "ca_m_3412037_nw_10_1_20140603_20141030      50837\n",
      "ca_m_3412037_nw_10_1_20120518_20120730      31957\n",
      "ca_m_3412037_nw_10_060_20180913_20190208    28251\n",
      "ca_m_3411936_se_11_060_20200521             27930\n",
      "ca_m_3411934_sw_11_060_20200521             25293\n",
      "ca_m_3411936_se_11_060_20180724_20190209    23820\n",
      "ca_m_3411934_sw_11_.6_20160713_20161004     20278\n",
      "ca_m_3411936_se_11_.6_20160713_20161004     18413\n",
      "ca_m_3411934_sw_11_1_20120505_20120730      18378\n",
      "ca_m_3411936_se_11_1_20120505_20120730      16568\n",
      "ca_m_3411936_se_11_1_20140901_20141030      15358\n",
      "ca_m_3412039_nw_10_060_20200522             15246\n",
      "ca_m_3412039_nw_10_060_20180724_20190209    14886\n",
      "ca_m_3411934_sw_11_1_20140601_20141030      13432\n",
      "ca_m_3412039_nw_10_.6_20160616_20161004     13053\n",
      "ca_m_3412039_nw_10_1_20120518_20120730      10321\n",
      "ca_m_3412039_nw_10_1_20140603_20141030       9536\n",
      "\n",
      "\n",
      "***** INFORMATION ABOUT TRAIN/TEST DATASETS ******\n",
      "\n",
      "\n",
      "30.0% of points were included in test set\n",
      "sampling was stratified by NAIP scene\n",
      "# of points in train set: 342583\n",
      "# of points in test set: 146832\n"
     ]
    }
   ],
   "source": [
    "sep = '\\n\\n\\n'\n",
    "title1 = '***** INFORMATION ABOUT COMPLETE DATASET ******' +sep\n",
    "# ----------------------------\n",
    "\n",
    "n_sample =  'total # of points sampled: '+ str(samples.shape[0]) + sep\n",
    "\n",
    "# ----------------------------\n",
    "# ratios and percentages of iceplant vs non-iceplant\n",
    "unique, counts = np.unique(samples.iceplant, return_counts=True)\n",
    "icep_ratio = 'no-iceplant:iceplant ratio   '+ str(round(counts[0]/counts[1],1))+':1' +sep\n",
    "\n",
    "n = samples.iceplant.shape[0]\n",
    "perc = [round(counts[0]/n*100,2), round(counts[1]/n*100,2)]\n",
    "counts_percents = pd.DataFrame({'iceplant':unique,\n",
    "         'counts':counts,\n",
    "         'percentage':perc}).set_index('iceplant')\n",
    "counts_percents = counts_percents.to_string() + sep\n",
    "\n",
    "# ----------------------------\n",
    "# Number of points by area of interest\n",
    "counts_aoi = 'Points sampled per area of interest\\n' + samples.aoi.value_counts().to_string() + sep\n",
    "\n",
    "# Number of points by year\n",
    "counts_year = 'Points sampled per year\\n'+ samples.year.value_counts().to_string() + sep\n",
    "\n",
    "# Number of points by NAIP scene\n",
    "counts_naipid = '# NAIP scenes sampled: '+ str(len(samples.naip_id.value_counts()))+'\\nPoints sampled per NAIP scene\\n' +  samples.naip_id.value_counts().to_string() + sep\n",
    "    \n",
    "\n",
    "# ----------------------------\n",
    "# assemble all stats into string\n",
    "stats = title1 + n_sample + icep_ratio + counts_percents + counts_aoi + counts_year + counts_naipid\n",
    "\n",
    "# ----------------------------\n",
    "if split == True:\n",
    "    title2 = '***** INFORMATION ABOUT TRAIN/TEST DATASETS ******' +sep\n",
    "    size = str(test_size*100) + '% of points were included in test set\\nsampling was stratified by NAIP scene\\n'\n",
    "    n_train =  '# of points in train set: '+ str(train.shape[0]) + '\\n'\n",
    "    n_test = '# of points in test set: '+ str(test.shape[0])\n",
    "    stats = stats +title2+ size + n_train + n_test\n",
    "\n",
    "# *******************************************************************************\n",
    "if write_stats:\n",
    "    with open(os.path.join(os.getcwd(),'samples_for_model_stats.txt'), 'w') as f:\n",
    "        f.write(stats)\n",
    "        f.close()\n",
    "\n",
    "# *******************************************************************************\n",
    "if verbose:\n",
    "    print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
