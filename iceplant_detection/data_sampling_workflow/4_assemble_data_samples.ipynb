{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa89fce3-0450-4190-a2ba-3af09e400f3e",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook assembles all the csv files produced by the `3_add_lidar.ipynb` notebook into a single dataframe of samples for iceplant detection model. While assembling the files together we also:\n",
    "- delete from the dataset any sample points that have a negative canopy height feature. Canopy height features are: lidar, max_lidar, min_lidar, avg_lidar and min_max_diff. \n",
    "- add NaN as the value for all canopy height features for points sampled on years 2012 and 2014. \n",
    "To run the notebook it is necessary to have a csv file with points for each year and area of interest. \n",
    "The output is either a single csv file with all the combined data samples, or two csv files corresponding to splitting the samples into train and test sets.  The training set is done by sampling the same specified percentage of points per scene. This is an effort to keep the training and test sets unbiased towards scenes that have more points sampled from them. \n",
    "The notebook also prints and saves some statistics of the resulting dataset(s).\n",
    "\n",
    "**VARIABLES**\n",
    "\n",
    "`delete_files` (bool): whether to delete the individual csvs from which the final dataset is assembled\n",
    "\n",
    "`verbose` (bool): whether to print in console the stats of how many points were sampled per year and aoi\n",
    "\n",
    "`write_stats` (bool):  whether to save as a text file stats about the distribution of sample points in the dataset(s)\n",
    "\n",
    "`split` (bool): if split=True, then the dataset is split into train and test sets, keeping the same proporiton of points per scene. \n",
    "\n",
    "`test_size` (float in (0,1)): percentage of data samples that should go into test set. The notebook will sample this percentage of test points from each scene. \n",
    "\n",
    "Add: new variables\n",
    "\n",
    "**OUTPUT:**\n",
    "- If split=False: a single csv file named 'samples_for_model.csv'. This contains all the points with all the features sampled. \n",
    "- If split=True: two csv files, one for the train set and another for the test set.\n",
    "- If write_stats=True: A text file named samples_for_model_stats.txt. Nothing is generated if write_stats=False.\n",
    "\n",
    "All files are saved in the current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a7815d-4fe8-4c41-b3d6-9c10af6995e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e5fbb-1a44-4b8c-b7d4-0df68eef2e10",
   "metadata": {},
   "source": [
    "# Specify notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83bca10-f788-490f-9cff-bd1fc5a5cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# ************* NOTEBOOK VARIABLES ******************\n",
    "\n",
    "delete_files = True\n",
    "\n",
    "# print stats as notebook runs\n",
    "verbose = True\n",
    "\n",
    "# save stats\n",
    "write_stats = True\n",
    "\n",
    "split = True\n",
    "\n",
    "test_size = 0.3\n",
    "# ***************************************************\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0ce63-481a-4fd0-a54c-a73713a447f1",
   "metadata": {},
   "source": [
    "## Paths to sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419e2adc-a221-4f89-9cf9-8228db7a6b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_spectral_points_csv(aoi, year):\n",
    "    \"\"\" Assembles a file path to file with points and ONLY spectral information from given aoi and year. \"\"\"\n",
    "    fp = os.path.join(os.getcwd(), \n",
    "                      'temp',\n",
    "                      aoi +'_points_'+str(year)+'.csv')\n",
    "    return fp            \n",
    "\n",
    "#---------------------\n",
    "\n",
    "def path_lidar_spectral_points_csv(aoi, year):\n",
    "    \"\"\" Assembles a file path to file with points and canopy height AND spectral information from given aoi and year. \"\"\"\n",
    "    fp = os.path.join(os.getcwd(), \n",
    "                      'temp',\n",
    "                      aoi +'_pts_spectral_lidar_'+str(year)+'.csv')\n",
    "    return fp            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d4a1a-7f56-4aec-8566-ddac8a30bf28",
   "metadata": {},
   "source": [
    "## Assemble data frame with all sampled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3395de-90d4-4d59-a897-c5ca04a504d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lidar_years = [2016,2018,2020]\n",
    "spec_years = [2012,2014]\n",
    "aois = ['campus_lagoon','carpinteria','gaviota','point_conception']\n",
    "\n",
    "# *******************************************************************************\n",
    "# Open and concatenate csv files of points with canopy height + spectral info\n",
    "li = []\n",
    "for aoi in aois:\n",
    "    for year in lidar_years:\n",
    "        if ('point_conception' != aoi) or (year != 2016):  #there's no data for Point Conception on 2016\n",
    "            sample = pd.read_csv(path_lidar_spectral_points_csv(aoi,year))\n",
    "            li.append(sample)\n",
    "\n",
    "df_lidar = pd.concat(li, axis=0)\n",
    "\n",
    "# only keep points with non-negative canopy height values\n",
    "df_lidar = df_lidar.loc[(df_lidar[\"lidar\"] >= 0) & \n",
    "                        (df_lidar[\"max_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"min_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"avg_lidar\"] >= 0) &\n",
    "                        (df_lidar[\"min_max_diff\"] >= 0)\n",
    "                       ]\n",
    "\n",
    "# *******************************************************************************\n",
    "# Open and concatenate csv files of points with canopy height + spectral info\n",
    "li = []\n",
    "for aoi in aois:\n",
    "    for year in spec_years:\n",
    "            sample = pd.read_csv(path_spectral_points_csv(aoi,year))\n",
    "            li.append(sample)\n",
    "\n",
    "df_spec = pd.concat(li, axis=0)\n",
    "\n",
    "# fill in canopy height columns wit NaN\n",
    "df_spec['lidar'] = np.nan\n",
    "df_spec['max_lidar'] = np.nan\n",
    "df_spec['min_lidar'] = np.nan\n",
    "df_spec['avg_lidar'] = np.nan\n",
    "df_spec['min_max_diff'] = np.nan\n",
    "\n",
    "# *******************************************************************************\n",
    "# concatenate both data frames and clean index and columns\n",
    "samples = pd.concat([df_lidar,df_spec], axis=0)\n",
    "\n",
    "samples.reset_index(drop=True, inplace=True)\n",
    "\n",
    "samples = samples[['x', 'y', 'pts_crs', #  point location\n",
    "         'aoi','naip_id', 'polygon_id',  # sampling info\n",
    "         'r','g','b','nir','ndvi',     # spectral\n",
    "         'year','month','day_in_year', # date\n",
    "         'lidar', 'max_lidar', 'min_lidar', 'min_max_diff', 'avg_lidar', # lidar\n",
    "         'iceplant'\n",
    "         ]] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73390f-fff9-4cbc-a935-92112e197b0c",
   "metadata": {},
   "source": [
    "## If split == True: split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53cfde5-6271-4034-8c3f-14b18a2e8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    # initialize empty train and test lists\n",
    "    all_train = []\n",
    "    all_test = []\n",
    "\n",
    "    X_labels = samples.columns.drop('iceplant')     # save label names\n",
    "\n",
    "    aois = samples.aoi.unique()     # list of aois\n",
    "\n",
    "    for aoi in aois:\n",
    "\n",
    "        # retrieve all scenes from  aoi\n",
    "        in_aoi = samples[samples.aoi == aoi]    \n",
    "        scenes = in_aoi.naip_id.unique()\n",
    "\n",
    "        for scene in scenes:\n",
    "            # get all pts in scene\n",
    "            in_scene = in_aoi[in_aoi.naip_id == scene]\n",
    "\n",
    "            # sample test_size fraction of pts in scene for testing\n",
    "            # keep same iceplant/non-ice plant proportion in test an train sets\n",
    "            X = np.array(in_scene.drop('iceplant', axis = 1))\n",
    "            y = np.array(in_scene['iceplant'])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                                test_size = test_size, \n",
    "                                                                random_state = 42)   # fix random seed\n",
    "            # reassemble train set into data frame\n",
    "            train = pd.DataFrame(X_train, columns = X_labels)\n",
    "            train['iceplant'] = y_train\n",
    "\n",
    "            # reassemble test set into data \n",
    "            test = pd.DataFrame(X_test, columns = X_labels)\n",
    "            test['iceplant'] = y_test\n",
    "\n",
    "            # add to rest of train/test pts\n",
    "            all_train.append(train)\n",
    "            all_test.append(test)\n",
    "    \n",
    "    train = pd.concat(all_train, ignore_index=True)\n",
    "    test = pd.concat(all_test, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9107185-373d-4ead-bdf1-d5a2f90179e3",
   "metadata": {},
   "source": [
    "## Save data samples file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31059e58-8108-4e37-a3d6-5fc0c07024d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    train.to_csv(os.path.join(os.getcwd(),'train_set.csv'), index=False)\n",
    "    test.to_csv(os.path.join(os.getcwd(),'test_set.csv'), index=False)\n",
    "    \n",
    "else:\n",
    "    samples.to_csv(os.path.join(os.getcwd(),'samples_for_model.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ef100-2d99-4a8f-85a8-98ca9ac49882",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Delete individual csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2082a9e8-3c7b-47d7-9617-bbba3f28cc76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/msai4earth-esa/iceplant_detection/data_sampling_workflow/temp/campus_lagoon_pts_spectral_lidar_2016.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m lidar_years:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoint_conception\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m!=\u001b[39m aoi) \u001b[38;5;129;01mor\u001b[39;00m (year \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2016\u001b[39m):  \u001b[38;5;66;03m#there's no data for Point Conception on 2016\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_lidar_spectral_points_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43maoi\u001b[49m\u001b[43m,\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/msai4earth-esa/iceplant_detection/data_sampling_workflow/temp/campus_lagoon_pts_spectral_lidar_2016.csv'"
     ]
    }
   ],
   "source": [
    "if delete_files == True:\n",
    "    for aoi in aois:\n",
    "        for year in [2012,2014,2016,2018,2020]:\n",
    "            if year in spec_years:\n",
    "                os.remove(path_spectral_points_csv(aoi,year))\n",
    "            if year in lidar_years:\n",
    "                if ('point_conception' != aoi) or (year != 2016):  #there's no data for Point Conception on 2016\n",
    "                    os.remove(path_lidar_spectral_points_csv(aoi,year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24726e-83a8-4c5e-8f22-b863d89cbafd",
   "metadata": {},
   "source": [
    "## Statistics about data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2138e-3c47-4c5f-85ef-95a659d0c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = '\\n\\n\\n'\n",
    "title1 = '***** INFORMATION ABOUT COMPLETE DATASET ******' +sep\n",
    "# ----------------------------\n",
    "\n",
    "n_sample =  'total # of points sampled: '+ str(samples.shape[0]) + sep\n",
    "\n",
    "# ----------------------------\n",
    "# ratios and percentages of iceplant vs non-iceplant\n",
    "unique, counts = np.unique(samples.iceplant, return_counts=True)\n",
    "icep_ratio = 'no-iceplant:iceplant ratio   '+ str(round(counts[0]/counts[1],1))+':1' +sep\n",
    "\n",
    "n = samples.iceplant.shape[0]\n",
    "perc = [round(counts[0]/n*100,2), round(counts[1]/n*100,2)]\n",
    "counts_percents = pd.DataFrame({'iceplant':unique,\n",
    "         'counts':counts,\n",
    "         'percentage':perc}).set_index('iceplant')\n",
    "counts_percents = counts_percents.to_string() + sep\n",
    "\n",
    "# ----------------------------\n",
    "# Number of points by area of interest\n",
    "counts_aoi = 'Points sampled per area of interest\\n' + samples.aoi.value_counts().to_string() + sep\n",
    "\n",
    "# Number of points by year\n",
    "counts_year = 'Points sampled per year\\n'+ samples.year.value_counts().to_string() + sep\n",
    "\n",
    "# Number of points by NAIP scene\n",
    "counts_naipid = '# NAIP scenes sampled: '+ str(len(samples.naip_id.value_counts()))+'\\nPoints sampled per NAIP scene\\n' +  samples.naip_id.value_counts().to_string() + sep\n",
    "    \n",
    "\n",
    "# ----------------------------\n",
    "# assemble all stats into string\n",
    "stats = title1 + n_sample + icep_ratio + counts_percents + counts_aoi + counts_year + counts_naipid\n",
    "\n",
    "# ----------------------------\n",
    "if split == True:\n",
    "    title2 = '***** INFORMATION ABOUT TRAIN/TEST DATASETS ******' +sep\n",
    "    size = str(test_size*100) + '% of points were included in test set\\nsampling was stratified by NAIP scene\\n'\n",
    "    n_train =  '# of points in train set: '+ str(train.shape[0]) + '\\n'\n",
    "    n_test = '# of points in test set: '+ str(test.shape[0])\n",
    "    stats = stats +title2+ size + n_train + n_test\n",
    "\n",
    "# *******************************************************************************\n",
    "if write_stats:\n",
    "    with open(os.path.join(os.getcwd(),'samples_for_model_stats.txt'), 'w') as f:\n",
    "        f.write(stats)\n",
    "        f.close()\n",
    "\n",
    "# *******************************************************************************\n",
    "if verbose:\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0a95a-36d7-4572-862a-0d4b2dd37cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
