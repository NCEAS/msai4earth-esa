{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47801f9-46e8-4bb2-8980-dbc487f35af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import geopandas as gpd\n",
    "import rioxarray as rioxr\n",
    "import rasterio\n",
    "\n",
    "#from skimage.filters.rank import entropy\n",
    "import sample_rasters as sr\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "from shapely.geometry import box\n",
    "\n",
    "import planetary_computer as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d916444f-1f0b-42e6-8f06-fcd8903eaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# ************* NOTEBOOK VARIABLES ******************\n",
    "\n",
    "itemids = pd.read_csv(os.path.join(os.getcwd(),'temp', 'aoi_naip_itemids.csv'))\n",
    "\n",
    "csv_name = 'test_set.csv'\n",
    "all_pts = pd.read_csv(os.path.join(os.getcwd(), csv_name))\n",
    "\n",
    "# ***************************************************\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca68c8c9-5678-45b0-9f85-656d8a7336ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary folder for aux rasters\n",
    "folp = os.path.join(os.getcwd(),'temp','aux_naip_rasters')\n",
    "if os.path.exists(folp) == False:\n",
    "    os.mkdir(folp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff94901d-5c85-4799-9a16-3ec2e7025172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0929617881774902\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "sampled_pts = []\n",
    "\n",
    "for i in [0]:\n",
    "#for i in range(len(itemids)):\n",
    "    # ---------------------------------------\n",
    "    # open raster reader for NAIP scene\n",
    "    itemid = itemids.itemid[i]\n",
    "    item = sr.get_item_from_id(itemid)    \n",
    "    href = pc.sign(item.assets[\"image\"].href)\n",
    "    naip_rast_r = rioxr.open_rasterio(href) \n",
    "\n",
    "    # ---------------------------------------\n",
    "    # find polygons for that NAIP scene\n",
    "    poly_fp = sr.path_to_polygons(itemids.iloc[i].aoi_name, itemids.iloc[i].year)\n",
    "    polys = gpd.read_file(poly_fp)\n",
    "    \n",
    "    # iterate through polygons in scene\n",
    "    for j in [0]:\n",
    "#    for j in list(polys.id):\n",
    "        # ---------------------------------------\n",
    "        # find points in current polygon\n",
    "        pts_poly = all_pts.loc[ (all_pts['naip_id'] == itemid) & (all_pts['polygon_id'] == j)]\n",
    "        if len(pts_poly) !=0:\n",
    "            crs = CRS.from_string(pts_poly.pts_crs.iloc[0])\n",
    "            pts_poly_df = sr.geodataframe_from_csv(df = pts_poly, lon_label='x', lat_label='y', crs=crs)\n",
    "            pts_col = pts_poly_df.to_crs(naip_rast_r.rio.crs).geometry\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # create enlarged bounding box\n",
    "\n",
    "            # (this is ugly, but unfortunately the index and the polygon.id do not match in some files)\n",
    "            poly_index = polys.index[polys['id']==j].tolist()[0]                \n",
    "\n",
    "            poly = polys.geometry[poly_index]\n",
    "            reduce = gpd.GeoDataFrame({'geometry':[box(*poly.bounds)]}, crs=polys.crs)\n",
    "            reduce = reduce.to_crs(naip_rast_r.rio.crs) \n",
    "            poly = reduce.geometry[0]  # poly in scene's crs\n",
    "            reduce_box = box(*(poly.buffer(6).bounds)) # n must be 2*(entropy disk radius)\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # clip NAIP scene\n",
    "            rast = naip_rast_r.rio.clip_box(*reduce_box.bounds)\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # save auxiliary entropy rasters for R,G,B,NIR bands of clipped scene\n",
    "            band_names = ['r_', 'g_', 'b_', 'nir_']\n",
    "            tags = ['_maxs', '_mins', '_avgs', '_entrs']\n",
    "            window_fps = []\n",
    "            window_cols = []\n",
    "\n",
    "            for band_name, band_n in zip(band_names,range(1,5)):\n",
    "                rast_name = band_name + itemid + '_poly_'+str(j)\n",
    "                sr.max_raster(raster = rast, band=band_n, rast_name=rast_name, n=3, folder_path=folp)\n",
    "                sr.min_raster(raster = rast, band=band_n, rast_name=rast_name, n=3, folder_path=folp)\n",
    "                sr.avg_raster(raster = rast, band=band_n, rast_name=rast_name, n=3, folder_path=folp)\n",
    "                sr.entropy_raster(raster = rast, band=band_n, rast_name=rast_name, n=3, folder_path=folp)\n",
    "\n",
    "                for tag in tags:\n",
    "                    window_fps.append(os.path.join(folp, rast_name + tag + '.tif'))        \n",
    "                    window_cols.append( band_name.replace('_','')+tag.replace('s',''))\n",
    "\n",
    "            # ------------------------------\n",
    "            # make auxiliary NDVI of clipped scene\n",
    "            red_band = rast.sel(band=1).astype('int16') \n",
    "            nir_band = rast.sel(band=4).astype('int16')\n",
    "            ndvi = ((nir_band - red_band) / (nir_band + red_band)*100)+100\n",
    "\n",
    "            # make auxiliary NDVI entropy\n",
    "            band_names.append('ndvi_')\n",
    "            rast_name = 'ndvi_' + itemid + '_poly_'+str(j)\n",
    "            \n",
    "            sr.max_raster(rast_data=ndvi, crs=rast.rio.crs, transf=rast.rio.transform(), rast_name=rast_name, n=3, folder_path=folp)\n",
    "            sr.min_raster(rast_data=ndvi, crs=rast.rio.crs, transf=rast.rio.transform(), rast_name=rast_name, n=3, folder_path=folp)\n",
    "            sr.avg_raster(rast_data=ndvi, crs=rast.rio.crs, transf=rast.rio.transform(), rast_name=rast_name, n=3, folder_path=folp)\n",
    "\n",
    "            ndvi = ndvi*100 +100\n",
    "            sr.entropy_raster(rast_data=ndvi.astype('uint8'), \n",
    "                              crs=rast.rio.crs, transf=rast.rio.transform(), \n",
    "                              rast_name=rast_name, n=3, \n",
    "                              folder_path=folp)\n",
    "\n",
    "            for tag in tags:\n",
    "                window_fps.append(os.path.join(folp, rast_name + tag + '.tif'))        \n",
    "                window_cols.append( 'ndvi'+tag.replace('s',''))\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # sample raster values for points in current polygon\n",
    "            samples = []\n",
    "            for fp, col_name in zip(window_fps, window_cols):\n",
    "                rast_r = rasterio.open(fp)\n",
    "                sample = sr.sample_raster_from_pts(pts_col, rast_r, [col_name])    \n",
    "                samples.append(sample)\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Add all derived spectral data to pts dataframe\n",
    "            new_features = pd.concat(samples, axis = 1)\n",
    "            pts = pd.concat([pts_poly, new_features.set_index(pts_poly_df.index)], axis=1)                \n",
    "\n",
    "            # -----------------------------\n",
    "            # collect all points from each polygon in the scene\n",
    "            sampled_pts.append(pts)\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # delete aux entropy rasters\n",
    "            for fp in window_fps:\n",
    "                os.remove(fp)\n",
    "            \n",
    "sampled_pts= pd.concat(sampled_pts).sort_index()\n",
    "        \n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d70950-002f-4b2f-a03d-3bce1668f33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x', 'y', 'pts_crs', 'aoi', 'naip_id', 'polygon_id', 'r', 'g', 'b',\n",
       "       'nir', 'ndvi', 'year', 'month', 'day_in_year', 'lidar', 'max_lidar',\n",
       "       'min_lidar', 'min_max_diff', 'avg_lidar', 'iceplant', 'geometry',\n",
       "       'r_max', 'r_min', 'r_avg', 'r_entr', 'g_max', 'g_min', 'g_avg',\n",
       "       'g_entr', 'b_max', 'b_min', 'b_avg', 'b_entr', 'nir_max', 'nir_min',\n",
       "       'nir_avg', 'nir_entr', 'ndvi_max', 'ndvi_min', 'ndvi_avg', 'ndvi_entr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_pts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb13333-8b1f-4302-9ae5-05dc2263b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = 'spectral_window_test_set.csv'\n",
    "win_pts =  pd.read_csv(os.path.join(os.getcwd(), csv_name))\n",
    "\n",
    "sub = win_pts.loc[ (win_pts['naip_id'] == itemid) & (win_pts['polygon_id'] == j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7483a8a-592a-4373-b0a0-19eb79e0f1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167000    150\n",
       "167119    142\n",
       "167125    144\n",
       "167127    144\n",
       "167144    150\n",
       "         ... \n",
       "198039    140\n",
       "198056    144\n",
       "198162    141\n",
       "198182    143\n",
       "198357    145\n",
       "Name: r_max, Length: 568, dtype: uint8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_pts.r_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7f6fb4e-ec5a-486b-beb0-5da7ce624360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for col in window_cols:\n",
    "    print(list(sub[col]) == list(sampled_pts[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca500422-e37c-4f53-a264-54dfa300e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = sampled_pts.r_avg.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86ffbd4f-5687-48c9-af01-96d93457659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = sub.r_avg.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b583236-368e-4e52-9c8b-2ced040476fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(new - old < 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236ccf4-a779-4858-a908-fb0ae3a9cd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
