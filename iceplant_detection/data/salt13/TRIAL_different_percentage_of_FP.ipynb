{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45523e1e-1d79-40f2-b6cd-e5286fb56a02",
   "metadata": {},
   "source": [
    "In this notebook I add the FP from greentea model into the green tea train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4ebe97-f0a1-44fe-89f3-a53b74f875d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c564390-5534-460f-8a04-195f98afd2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_info_df(y_true, y_pred):\n",
    "    #N = y_true.shape[0]\n",
    "\n",
    "    unique, counts = np.unique(y_true,return_counts=True)    \n",
    "    N = counts[0]    \n",
    "    P = counts[1]\n",
    "    \n",
    "    confmtx = confusion_matrix(y_true, y_pred)\n",
    "    TN = confmtx[0,0]\n",
    "    FP = confmtx[0,1]\n",
    "    FN = confmtx[1,0]\n",
    "    TP = confmtx[1,1]\n",
    "\n",
    "    # P's  producer's accuracy (sensitivity) : TP/P\n",
    "    PA_P =  np.round( TP/P  * 100, 2) \n",
    "\n",
    "    # N's producer's accuracy (specificity) : TN/N\n",
    "    PA_N =  np.round( TN/N * 100, 2) \n",
    "\n",
    "    # P's user's accuracy (precision P) : TP/(TP+FP)\n",
    "    UA_P = np.round( TP / (TP+FP) * 100, 2) \n",
    "    \n",
    "    # N's user's accuracy (precision N) : TN/(TN+FN)\n",
    "    UA_N = np.round( TN / (TN+FN) * 100, 2)\n",
    "    \n",
    "    # overal accuracy: (TP + TN)/(P + N)\n",
    "    OA = np.round( (TP+TN)/y_true.shape[0]*100, 2) \n",
    "    \n",
    "    D = {'acc':OA,\n",
    "         'prod_acc_P':PA_P, \n",
    "         'prod_acc_N':PA_N,          \n",
    "         'user_acc_P':UA_P,\n",
    "         'user_acc_N':UA_N,         \n",
    "         'TP':TP, 'TN':TN, \n",
    "         'FP':FP, 'FN':FN \n",
    "        }\n",
    "    df = pd.DataFrame([D])\n",
    "    return df\n",
    "# ----------------------------------------------------------------\n",
    "cols = ['r', \n",
    "        'r_avg13', 'r_entr13',         \n",
    "        'g',\n",
    "        'g_avg13', 'g_entr13',                 \n",
    "        'b',\n",
    "        'b_avg13', 'b_entr13',                 \n",
    "        'nir',\n",
    "        'nir_avg13', 'nir_entr13',                 \n",
    "        'ndvi',\n",
    "        'ndvi_avg13', 'ndvi_entr13',        \n",
    "        'month', \n",
    "        'day_in_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e32da3-13db-47dc-a9a0-8b0631f1fcde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([False]), array([760])) \n",
      "\n",
      "(array([False]), array([5974])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('/home/jovyan/msai4earth-esa/iceplant_detection/data/greentea13/greentea13_test.csv')\n",
    "train = pd.read_csv('/home/jovyan/msai4earth-esa/iceplant_detection/data/greentea13/greentea13_train.csv')\n",
    "\n",
    "fp = pd.read_csv('greentea_FP.csv')\n",
    "\n",
    "# check duplicates in false positives and in merged set\n",
    "print(np.unique(fp.duplicated(), return_counts=True), '\\n')\n",
    "print(np.unique(pd.concat([fp, train, test])[['x','y']].duplicated(), return_counts=True), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67b4a85-e6ac-4913-859f-b3a2bbe91c69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# distribute false positives into train and test\n",
    "\n",
    "aois = ['gaviota','capitan','campus_lagoon','carpinteria']\n",
    "percentages = [100, 90, 80, 70, 60, 50, 40, 30, 20, 10]\n",
    "\n",
    "results = []\n",
    "rand_state = 80\n",
    "for perc in percentages:\n",
    "\n",
    "    test_samples = []\n",
    "    train_samples = []\n",
    "    \n",
    "    for aoi in aois:\n",
    "\n",
    "        # select false positives in that aoi\n",
    "        df = fp[fp.aoi == aoi].sample(frac=perc/100, random_state=rand_state)\n",
    "\n",
    "        # sample 70% of these for train set\n",
    "        xtr_train = df.sample(frac=0.7, random_state=rand_state)\n",
    "        train_samples.append(xtr_train)\n",
    "\n",
    "        # pick remaining 30% for test set\n",
    "        xtr_test = df.loc[list(set(df.index) - set(xtr_train.index))]\n",
    "        test_samples.append(xtr_test)\n",
    "\n",
    "    fp_test = pd.concat(test_samples)\n",
    "    fp_train = pd.concat(train_samples)\n",
    "            \n",
    "    salt_test =  pd.concat([fp_test, test])\n",
    "    salt_train =  pd.concat([fp_train, train])\n",
    "\n",
    "    # ------------------------------    \n",
    "    # divide into train and test sets \n",
    "    X_test = salt_test[cols].to_numpy()\n",
    "    y_test = salt_test.loc[:,'iceplant'].to_numpy()\n",
    "    \n",
    "    X_train = salt_train[cols].to_numpy() \n",
    "    y_train = salt_train.loc[:,'iceplant'].to_numpy() \n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    preds = rfc.predict(X_test)\n",
    "    results.append(accuracy_info_df(y_test, preds))\n",
    "\n",
    "\n",
    "# check counts\n",
    "# print(fp_test.groupby(['aoi']).count().x, '\\n')\n",
    "\n",
    "# print(fp_train.groupby(['aoi']).count().x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "373d193c-6989-4d14-916d-2414fee6ab69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>acc</th>\n",
       "      <th>prod_acc_P</th>\n",
       "      <th>prod_acc_N</th>\n",
       "      <th>user_acc_P</th>\n",
       "      <th>user_acc_N</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>82.36</td>\n",
       "      <td>58.56</td>\n",
       "      <td>95.18</td>\n",
       "      <td>86.73</td>\n",
       "      <td>81.01</td>\n",
       "      <td>366</td>\n",
       "      <td>1105</td>\n",
       "      <td>56</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>82.14</td>\n",
       "      <td>58.88</td>\n",
       "      <td>94.91</td>\n",
       "      <td>86.38</td>\n",
       "      <td>80.79</td>\n",
       "      <td>368</td>\n",
       "      <td>1081</td>\n",
       "      <td>58</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>81.72</td>\n",
       "      <td>59.84</td>\n",
       "      <td>93.99</td>\n",
       "      <td>84.81</td>\n",
       "      <td>80.68</td>\n",
       "      <td>374</td>\n",
       "      <td>1048</td>\n",
       "      <td>67</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>83.59</td>\n",
       "      <td>64.32</td>\n",
       "      <td>94.60</td>\n",
       "      <td>87.20</td>\n",
       "      <td>82.26</td>\n",
       "      <td>402</td>\n",
       "      <td>1034</td>\n",
       "      <td>59</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>82.01</td>\n",
       "      <td>64.16</td>\n",
       "      <td>92.43</td>\n",
       "      <td>83.20</td>\n",
       "      <td>81.53</td>\n",
       "      <td>401</td>\n",
       "      <td>989</td>\n",
       "      <td>81</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>83.67</td>\n",
       "      <td>67.04</td>\n",
       "      <td>93.60</td>\n",
       "      <td>86.21</td>\n",
       "      <td>82.63</td>\n",
       "      <td>419</td>\n",
       "      <td>980</td>\n",
       "      <td>67</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>82.60</td>\n",
       "      <td>67.04</td>\n",
       "      <td>92.09</td>\n",
       "      <td>83.80</td>\n",
       "      <td>82.07</td>\n",
       "      <td>419</td>\n",
       "      <td>943</td>\n",
       "      <td>81</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>82.71</td>\n",
       "      <td>67.20</td>\n",
       "      <td>92.40</td>\n",
       "      <td>84.68</td>\n",
       "      <td>81.84</td>\n",
       "      <td>420</td>\n",
       "      <td>924</td>\n",
       "      <td>76</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>83.24</td>\n",
       "      <td>71.84</td>\n",
       "      <td>90.51</td>\n",
       "      <td>82.84</td>\n",
       "      <td>83.44</td>\n",
       "      <td>449</td>\n",
       "      <td>887</td>\n",
       "      <td>93</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>82.67</td>\n",
       "      <td>70.56</td>\n",
       "      <td>90.59</td>\n",
       "      <td>83.05</td>\n",
       "      <td>82.48</td>\n",
       "      <td>441</td>\n",
       "      <td>866</td>\n",
       "      <td>90</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percentage    acc  prod_acc_P  prod_acc_N  user_acc_P  user_acc_N   TP   \n",
       "0         100  82.36       58.56       95.18       86.73       81.01  366  \\\n",
       "1          90  82.14       58.88       94.91       86.38       80.79  368   \n",
       "2          80  81.72       59.84       93.99       84.81       80.68  374   \n",
       "3          70  83.59       64.32       94.60       87.20       82.26  402   \n",
       "4          60  82.01       64.16       92.43       83.20       81.53  401   \n",
       "5          50  83.67       67.04       93.60       86.21       82.63  419   \n",
       "6          40  82.60       67.04       92.09       83.80       82.07  419   \n",
       "7          30  82.71       67.20       92.40       84.68       81.84  420   \n",
       "8          20  83.24       71.84       90.51       82.84       83.44  449   \n",
       "9          10  82.67       70.56       90.59       83.05       82.48  441   \n",
       "\n",
       "     TN  FP   FN  \n",
       "0  1105  56  259  \n",
       "1  1081  58  257  \n",
       "2  1048  67  251  \n",
       "3  1034  59  223  \n",
       "4   989  81  224  \n",
       "5   980  67  206  \n",
       "6   943  81  206  \n",
       "7   924  76  205  \n",
       "8   887  93  176  \n",
       "9   866  90  184  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = pd.concat(results).reset_index(drop=True)\n",
    "R.insert(loc=0,\n",
    "         column = 'percentage',\n",
    "         value = percentages)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9853f8-0b82-4144-a13e-aa84513d6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aois = ['gaviota','capitan','campus_lagoon','carpinteria']\n",
    "# percentages = [40]\n",
    "\n",
    "# results = []\n",
    "# rand_state = 80\n",
    "# for perc in percentages:\n",
    "\n",
    "#     test_samples = []\n",
    "#     train_samples = []\n",
    "    \n",
    "#     for aoi in aois:\n",
    "\n",
    "#         # select false positives in that aoi\n",
    "#         df = fp[fp.aoi == aoi].sample(frac=perc/100, random_state=rand_state)\n",
    "\n",
    "#         # sample 70% of these for train set\n",
    "#         xtr_train = df.sample(frac=0.7, random_state=rand_state)\n",
    "#         train_samples.append(xtr_train)\n",
    "\n",
    "#         # pick remaining 30% for test set\n",
    "#         xtr_test = df.loc[list(set(df.index) - set(xtr_train.index))]\n",
    "#         test_samples.append(xtr_test)\n",
    "\n",
    "#     fp_test = pd.concat(test_samples)\n",
    "#     fp_train = pd.concat(train_samples)\n",
    "    \n",
    "#     # # double-check duplicates in merges\n",
    "#     print(perc)\n",
    "#     print(np.unique(pd.concat([fp_train, fp_test, train, test])[['x','y']].duplicated(), return_counts=True), '\\n')    \n",
    "        \n",
    "#     salt_test =  pd.concat([fp_test, test])\n",
    "#     salt_train =  pd.concat([fp_train, train])\n",
    "    \n",
    "#     salt_test.to_csv('salt_p'+str(perc)+'_test.csv', index=False)\n",
    "#     salt_train.to_csv('salt_p'+str(perc)+'_train.csv', index=False)    \n",
    "\n",
    "#     # ------------------------------    \n",
    "#     # divide into train and test sets \n",
    "#     X_test = salt_test[cols].to_numpy()\n",
    "#     y_test = salt_test.loc[:,'iceplant'].to_numpy()\n",
    "    \n",
    "#     X_train = salt_train[cols].to_numpy() \n",
    "#     y_train = salt_train.loc[:,'iceplant'].to_numpy() \n",
    "    \n",
    "#     rfc = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "#     rfc.fit(X_train, y_train)\n",
    "#     dump(rfc, 'salt_p'+str(perc)+'_rfc.joblib')\n",
    "    \n",
    "#     preds = rfc.predict(X_test)\n",
    "#     results.append(accuracy_info_df(y_test, preds))\n",
    "    \n",
    "# R = pd.concat(results).reset_index(drop=True)\n",
    "# R['percentage'] = percentages\n",
    "# R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1be9a0-ea46-4ac2-b69d-bea993f24450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
