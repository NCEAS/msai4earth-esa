{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "This is a notebook for calculating the following statistics in a map:\n",
    "- producer's accuracies and user's accuracies by class,\n",
    "- overall accuracy, and\n",
    "- area estimates. \n",
    "Each statistic is accompanied by a 95% confidence interval. \n",
    "To do this, we follow the notation and calculations in Olofsson et al. \n",
    "\n",
    "The data input needs to be:\n",
    "1. a csv of the points assessed with two columns: map_class and ref_class. Map class is the classification of the point in the map, ref_class is the \"ground truth\" classification of the point. \n",
    "\n",
    "2. a csv with the number of pixels per class in the map.\n",
    "\n",
    "Alternatiely, the notebook can be run using the confusion matrix (table 8) in the Olofsson et al. paper. \n",
    "\n",
    "## Reference\n",
    "\n",
    "Pontus Olofsson, Giles M. Foody, Martin Herold, Stephen V. Stehman, Curtis E. Woodcock, Michael A. Wulder, Good practices for estimating area and assessing accuracy of land change, Remote Sensing of Environment,Volume 148, 2014, Pages 42-57, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2014.02.015.  (https://www.sciencedirect.com/science/article/pii/S0034425714000704)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "To run the example in Olofsson et al., do not import any data and do not execute the cells in this section. \n",
    "Instead, go to the next section and execute the cell having the Olofsson et al. data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load csv with validation points\n",
    "df = pd.read_csv(os.path.join(os.getcwd(),\n",
    "                              'validation_data_salt13_p30_map',\n",
    "                              'salt13_p30_map_and_reference_classes.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load counts of pixels per class in map\n",
    "pix_counts = pd.read_csv(os.path.join(os.getcwd(),\n",
    "                                                'validation_data_salt13_p30_map',\n",
    "                                                'salt13_p30_2020_combined_pixel_counts_total.csv'))\n",
    "pix_counts = pix_counts.to_numpy()[0]\n",
    "pix_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# counts by reference class\n",
    "print('Points in each reference class')\n",
    "print(np.unique(df.ref_class, return_counts=True), '\\n')\n",
    "\n",
    "# counts by map class: these should match the counts given by the stratified sample design\n",
    "print('Points in each map class')\n",
    "print(np.unique(df.map_class, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify number of classes in the map\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Here we create a confusion matrix $n$ such that \n",
    "\n",
    "$n_{i,j}$ = number of points predicted as $i$, known to be $j$, \n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$n_{i,j}$ = number of points that have map class as $i$ and reference class $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CALCULATE CONFUSION MATRIX FOR IMPORTED DATA\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "# using confusion_matrix directly we get a matrix C such that\n",
    "# C_{i,j} = known to be i, predicted as  j \n",
    "# The notation in the paper is \n",
    "# n_{i,j} = predicted as i, known to be j \n",
    "# so we need to take the transpose\n",
    "\n",
    "n = confusion_matrix(df.ref_class, df.map_class, labels=range(n_classes)).T\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 200000  150000 3200000 6450000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 66,   0,   5,   4],\n",
       "       [  0,  55,   8,  12],\n",
       "       [  1,   0, 153,  11],\n",
       "       [  2,   1,   9, 313]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USE EXAMPLE TO RUN NOTEBOOK\n",
    "\n",
    "# Olofsson et al. data - Table 8\n",
    "\n",
    "pix_counts = np.array([200000, 150000, 3200000, 6450000])\n",
    "print(pix_counts)\n",
    "\n",
    "n = np.array([\n",
    "    [66, 0,  5,   4],\n",
    "    [0,  55, 8,   12],\n",
    "    [1,  0,  153, 11],\n",
    "    [2,  1,  9,   313]])\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "Throughout the following, let $p_{ij}$ be the (true) fraction of the map that has map class $i$ and reference class $j$. \n",
    "\n",
    "### User's Accuracy\n",
    "The user's accuracy of class $i$ is the fraction of the area mapped as class $i$ that has reference class $i$, this is (Olofsson et al. eq 2):\n",
    "$$U_i = \\frac{p_{ii}}{p_{i\\cdot}}.$$\n",
    "This is equivalent to the precision of class $i$. For example, when there are two classes (positive and negative) the user's accuracy of the positive class is the same as the precision of the true class (TP/(TP + FP)).\n",
    "\n",
    "To estimate the $U_i$'s from the points sample we have that\n",
    "$$\\hat{U}_i = \\frac{\\hat{p}_{ii}}{\\hat{p}_{i\\cdot}},$$\n",
    "where $\\hat{p}_{ij}$ are the estimations of $p_{ij}$ from the sample. \n",
    "For stratified random sampling in which the sampling strata correspond to the map classes we have that\n",
    "$$\\hat{p}_{ij} = W_i \\frac{n_{ij}}{n_{i\\cdot}},$$\n",
    "where\n",
    "- $W_i$ is the fraction of the map's area classified  as class i,\n",
    "- $n_{ij}$ is number of points with map class $i$, known to be reference class $j$ (entires in the confusion matrix), and\n",
    "- $n_{i\\cdot}$ is the number of points with map class $i$ (row sums in confusion matrix).\n",
    "\n",
    "Notice that the user's accuracy can be simplifeid to\n",
    "$$\\hat{U}_i = \\frac{n_{ii}}{n_{i\\cdot}},$$\n",
    "this is the formula implemented in the code.\n",
    "\n",
    "For user's accuracy of map class $i$, the estiamted variance is (Olfosson et al. eq. 6):\n",
    "$$\\hat{V}(\\hat{U}_i) = \\frac{\\hat{U}_i (1-\\hat{U}_i)}{n_{i\\cdot}-1}.$$\n",
    "\n",
    "**NOTE:** We calculate the user's accuracies first since these are needed to calculate the approximate variance of the overall accuracy. \n",
    "\n",
    "### Variance, Standard Error & Confidence Intervals\n",
    "Recall that the square root of the estimated variance results in the standard error of the estimator. For example, in the case of the estimated overall accuracy of the map $\\hat{O}$ we have that $\\hat{S}(\\hat{O}) = \\sqrt{\\hat{V}(\\hat{O})}$ (see Olofsson et al. eq. 5). \n",
    "\n",
    "Also, the standard error is used to get confidence intervals for the estimated statistic:the 95% confidence interval is estimated as $\\hat{O} \\pm 1.96 \\hat{S}(\\hat{O}) = \\hat{O} \\pm 1.96\\  \\sqrt{\\hat{V}(\\hat{O})}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user's accuracies: [88.0, 73.33333333333333, 92.72727272727272, 96.3076923076923]\n",
      "user's accuracies confidence interval: [ 7.3663222  10.02429481  3.95425872  2.04287379]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# points in sample that had class i in map (predicted as i, any true class j)\n",
    "# these will also be used in overal accuracy and producer's accuracies\n",
    "n_idot = [sum(n[i,:]) for i in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "# estimated users' accuracy (precision for each class: TP/(TP+FP))\n",
    "U_hat = [n[i,i] / n_idot[i] for i in range(n_classes)]\n",
    "\n",
    "var_U_hat = [U_hat[i] * (1-U_hat[i])/(n_idot[i]-1) for i in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "print(\"user's accuracies:\", [x*100 for x in U_hat])\n",
    "print(\"user's accuracies confidence interval:\", 1.95*np.sqrt(var_U_hat)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overal Accuracy\n",
    "\n",
    "Let $O$ be the (true) accuracy of the map, and $\\hat{O}$ its estimation from the sample. Then, following Olofsson et al., section 4.3, we have that \n",
    "$$\\hat{O} = \\sum_{i=1}^q \\hat{p}_{ii},$$\n",
    "where $q$ is the number of classes in the map and $\\hat{p}_{ii}$ is the estimation of $p_{ij}$, the (true) fraction of the area in the map that was classified as class $i$ and has reference class $j$. \n",
    "\n",
    "For overall accuracy, the estimated variance is (Olofsson et al. eq 5):\n",
    "$$\\hat{V}(\\hat{O}) = \\sum_{i=1}^q \\frac{W_i^2 \\hat{U}_i (1-\\hat{U}_i)}{n_{i\\cdot}-1}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall accuracy: 94.65118881118882\n",
      "overall accuracy confidence interval: 1.8389313570398378 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total number of pixels in the map\n",
    "total_pix = sum(pix_counts)\n",
    "\n",
    "# list with the fractions of area in map mapped as each class\n",
    "W = [pix_counts[i]/ total_pix for i in range(n_classes)]      \n",
    "\n",
    "# -------------------------------------\n",
    "# overall accuracy\n",
    "O_hat = sum([W[i]*n[i,i]/n_idot[i] for i in range(n_classes)])\n",
    "print('overall accuracy:', O_hat*100)\n",
    "\n",
    "# -------------------------------------\n",
    "var_O_hat = sum([ W[i]**2 * U_hat[i] * (1-U_hat[i])/(n_idot[i]-1) for i in range(n_classes)])\n",
    "\n",
    "# std error of estimated overall accuracy\n",
    "print('overall accuracy confidence interval:', 1.95*np.sqrt(var_O_hat)*100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer's Accuracy\n",
    "The producer's accuracy of class $i$ is the fraction of the (true) area with reference class $i$ that is actually mapped as class $j$, this is (Olofsson et al. eq 3):\n",
    "$$P_j = \\frac{p_{jj}}{p_{\\cdot j}}.$$\n",
    "This is equivalent to the sensitiviy of class $j$. For example, when there are two classes (positive and negative) the producer's accuracy of the positive class is the same as the sensitivy of the true class (TP/(TP + FN)).\n",
    "\n",
    "To estimate the $P_i$'s from the points sample we have that\n",
    "$$\\hat{P}_j = \\frac{\\hat{p}_{jj}}{\\hat{p}_{\\cdot j}},$$\n",
    "where the $\\hat{p}_{ij}$ are as before.\n",
    "\n",
    "For the producer's accuracy of class $j$ the estimated variance is given by (Olofsson et al. eq 7):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{V}(\\hat{P}_j) = \n",
    "\\frac{1}{\\hat{N}_{\\cdot j}^2} \n",
    "\\left( \n",
    "\\frac{N_{j \\cdot}^2 (1 - \\hat{P}_j)^2 \\hat{U}_j (1-\\hat{U}_j)}{n_{j \\cdot} -1}  \n",
    "+\n",
    "\\hat{P}_j^2\n",
    "\\sum_{i\\neq j}^q \n",
    "\\frac{N_{i\\cdot}^2}{n_{i \\cdot} - 1} \n",
    "\\frac{n_{ij}}{n_{i \\cdot}} \n",
    "\\left( 1 - \\frac{n_{ij}}{n_{i \\cdot}} \\right)\n",
    "\\right),$$\n",
    "where\n",
    "- $N_{j \\cdot}$ is the number of pixels in the map with map class $j$,\n",
    "- $n_{j\\cdot}$ is the number of sample points with map class $j$, and\n",
    "- $\\hat{N}_{\\cdot j} = \\sum_{i=1}^q N_{i\\cdot}\\frac{n_{ij}}{n_{i\\cdot}}$ is the estimated number of pixels with reference class $j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producer's accuracies: [74.86614048308412, 84.71563981042654, 93.45089085796926, 96.16089928314558]\n",
      "producer's accuracies confidence interval: [21.22215374 25.31103589  3.41492981  1.82678542] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_hat_dotj = []\n",
    "# estimated producer's accuracy (sensitiviy for each class TP/(TP+FN))\n",
    "P_hat = []  \n",
    "\n",
    "for j in range(n_classes):\n",
    "    # list of p_hat_ij with fixed j\n",
    "    p_hat_ij = [ W[i]*n[i,j]/n_idot[i] for i in range(n_classes) ]\n",
    "    p_hat_dotj.append(sum(p_hat_ij))  # equation (9)\n",
    "\n",
    "\n",
    "P_hat= [ (W[j]*n[j,j]/n_idot[j]) / p_hat_dotj[j] for j in range(n_classes)]\n",
    "# -------------------------------------\n",
    "print(\"producer's accuracies:\", [x*100 for x in P_hat])\n",
    "\n",
    "# -------------------------------------\n",
    "# -------------------------------------\n",
    "# VARIANCE\n",
    "# notice N_jdot is pixel_counts[j]\n",
    "N_hat_cdotj = []\n",
    "for j in range(n_classes):\n",
    "    summands = [ pix_counts[i] * n[i,j]/n_idot[i] for i in range(n_classes)]\n",
    "    N_hat_cdotj.append(sum(summands))\n",
    "\n",
    "# -------------------------------------\n",
    "summand1 = [ (pix_counts[j]**2) * ((1-P_hat[j])**2) * U_hat[j] * (1-U_hat[j]) / (n_idot[j] - 1) \n",
    "            for j in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "summand2 = []\n",
    "for j in range(n_classes):\n",
    "    inner = []\n",
    "    for i in range(n_classes):\n",
    "        if i!=j:\n",
    "            inner.append( (pix_counts[i]**2) /(n_idot[i]-1) * (n[i,j])/(n_idot[i]) * ( 1 - n[i,j]/n_idot[i]) ) \n",
    "    summand2.append((P_hat[j]**2) * sum(inner))\n",
    "\n",
    "# -------------------------------------\n",
    "var_P_hat = [1/(N_hat_cdotj[j]**2) *  (summand1[j] + summand2[j]) for j in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "# -------------------------------------\n",
    "print(\"producer's accuracies confidence interval:\", 1.95*np.sqrt(var_P_hat)*100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For stratified random sampling when the map classes are the strata, we have that an estimator of the proportion of area of class $j$ is (Olofsson et al. eq. 9):\n",
    "$$ \\hat{p}_{\\cdot j} = \\sum_{i=1}^q W_i \\frac{n_{ij}}{n_{i\\cdot}}.$$\n",
    "\n",
    "For this estimator of area proportion per class, the standard error is estimated by (Olofsson et al. eq 10):\n",
    "$$S(\\hat{p}_{\\cdot j}) =  \n",
    "\\sqrt{\n",
    "\\sum_{i=1}^q W_i^2 \\frac{ \\frac{n_{ij}}{n_{i\\cdot}} \\left(1 -  \\frac{n_{ij}}{n_{i\\cdot}} \\right)}{n_{i \\cdot}-1}\n",
    "}.$$\n",
    "\n",
    "The estimated area of class $j$ is\n",
    "$$\\hat{A}_j = A \\times \\hat{p}_{\\cdot k},$$\n",
    "where $A$ is the total are of the map. \n",
    "The standard error for the area is given by (Olofsson et al. eq 11):\n",
    "$$ S(\\hat{A}_j) = A \\times S(\\hat{p}_{\\cdot j}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of area per class: \n",
      " [2.350862470862471, 1.2984615384615383, 31.752214452214456, 64.59846153846155]\n",
      "confidence interval for percentage area per class:\n",
      " [0.6841815984519081, 0.4173140028226505, 1.723315144243176, 1.8090729280271949]\n"
     ]
    }
   ],
   "source": [
    "# PERCENTAGE OF AREA ESTIMATION\n",
    "# we had calculated the area estimators before, they are used in producer's accuracy\n",
    "print(\"percentage of area per class: \\n\", [x*100 for x in p_hat_dotj])\n",
    "\n",
    "# -------------------------------------\n",
    "# STD ERROR\n",
    "SE_p_hat_dotj = []\n",
    "for j in range(n_classes):\n",
    "    summands = [ (W[i]**2) * (n[i,j]/n_idot[i]) * (1 -  (n[i,j]/n_idot[i]))/ (n_idot[i]-1) \n",
    "                for i in range(n_classes)]\n",
    "    SE_p_hat_dotj.append(np.sqrt(sum(summands)))\n",
    "    \n",
    "print(\"confidence interval for percentage area per class:\\n\", [x*1.96*100 for x in SE_p_hat_dotj])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx area per class (ha): \n",
      " [21158.02223803551, 11686.297453174257, 285773.4417957486, 581393.2982954193]\n",
      "confidence interval for area per class (m^2):\n",
      " [6157.710055063028, 3755.8721794243975, 15510.026893093216, 16281.856431773647]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "map_area = total_pix * (21158/235086)\n",
    "\n",
    "approx_area_per_class = [map_area * p_hat_dotj[i] for i in range(n_classes)]\n",
    "print(\"approx area per class (ha): \\n\", approx_area_per_class)\n",
    "\n",
    "SE_area_per_class = [map_area * SE_p_hat_dotj[i] for i in range(n_classes)]\n",
    "print(\"confidence interval for area per class (m^2):\\n\", [x*1.96 for x in SE_area_per_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # AREA ESTIMATION\n",
    "# # in m^2, assuming a resolution of 0.5m per pixel side\n",
    "# # (each pixel has an area of 0.25 m^2)\n",
    "# map_area = total_pix * 0.25 \n",
    "\n",
    "# approx_area_per_class = [map_area * p_hat_dotj[i] for i in range(n_classes)]\n",
    "# print(\"approx area per class (m^2): \\n\", approx_area_per_class)\n",
    "\n",
    "# SE_area_per_class = [map_area * SE_p_hat_dotj[i] for i in range(n_classes)]\n",
    "# print(\"confidence interval for area per class (m^2):\\n\", [x*1.96 for x in SE_area_per_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # in km^2, assuming a resolution of 0.5m per pixel side\n",
    "# map_area = total_pix * 0.25 / (1000**2)\n",
    "\n",
    "# approx_area_per_class = [map_area * p_hat_dotj[i] for i in range(n_classes)]\n",
    "# print(\"approx area per class (km^2): \\n\", approx_area_per_class)\n",
    "\n",
    "# SE_area_per_class = [map_area * SE_p_hat_dotj[i] for i in range(n_classes)]\n",
    "# print(\"confidence interval for area per class (km^2):\\n\", [x*1.96 for x in SE_area_per_class])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
